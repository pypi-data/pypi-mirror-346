{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67ZO0C_9u9Bb"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk   # Natural Language Tool Kit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLZY-teQvpBf",
        "outputId": "47d9ad5a-a69e-4702-fec7-e2a63b4518cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')    # punkt is the required package for tokenization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVFrsrX9wI0q",
        "outputId": "d1b56c9d-d0f7-452a-c8c0-613988ef8bf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', 'everyone', '!', ',', 'Welcome', 'to', 'my', 'blog', 'post', 'on', 'Medium', '.', 'We', 'are', 'studying', 'Natural', 'Language', 'Processing', '.']\n",
            "\n",
            "['Hello everyone!, Welcome to my blog post on Medium.', 'We are studying Natural Language Processing.']\n"
          ]
        }
      ],
      "source": [
        "# Tokenization\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "sent = \"Hello everyone!, Welcome to my blog post on Medium. We are studying Natural Language Processing.\"\n",
        "print(word_tokenize(sent))\n",
        "print()\n",
        "print(sent_tokenize(sent))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSR-itZuwsZ9",
        "outputId": "766f98fc-efcc-4063-c52b-88d03a854e88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# stopwords\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYMocpELw8AF",
        "outputId": "f12f7613-cb6f-462f-ea30-02fb708c3a04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the unclean version: ['Hello', 'everyone', '!', ',', 'Welcome', 'to', 'my', 'blog', 'post', 'on', 'Medium', '.', 'We', 'are', 'studying', 'Natural', 'Language', 'Processing', '.']\n",
            "This is the cleaned version: ['Hello', 'everyone', '!', ',', 'Welcome', 'blog', 'post', 'Medium', '.', 'We', 'studying', 'Natural', 'Language', 'Processing', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords        # the corpus module is an extremely useful one.\n",
        "\n",
        "stop_words = stopwords.words('english')  # this is the full list of all stop-words stored in nltk\n",
        "\n",
        "token = word_tokenize(sent)\n",
        "cleaned_token = []\n",
        "for word in token:\n",
        "    if word not in stop_words:\n",
        "        cleaned_token.append(word)\n",
        "print(\"This is the unclean version:\", token)\n",
        "print(\"This is the cleaned version:\", cleaned_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1QEI9zVxOUd",
        "outputId": "94d05d7b-6913-43ef-caa1-3f46862f1f27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['this', 'is', 'a', 'demo', 'text', 'for', 'nlp', 'use', 'nltk', '.', 'full', 'form', 'of', 'nltk', 'is', 'natur', 'languag', 'toolkit']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "text=\"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n",
        "print (stemmed_word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR5UsC2vxRbg",
        "outputId": "ab951fb5-165a-44cb-a74e-a008eb6f659d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i967nDafxUAV",
        "outputId": "a616eab9-2dca-4f41-8cc2-78846da5754e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['the', 'dog', 'are', 'barking', 'outside', '.', 'Are', 'the', 'cat', 'in', 'the', 'garden', '?']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "#is based on The Porter Stemming Algorithm\n",
        "stopword = stopwords.words('english')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "text = \"the dogs are barking outside. Are the cats in the garden?\"\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
        "print (lemmatized_word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhfIhs73xWfC",
        "outputId": "84f3d5a3-430e-4f4a-aebf-6cfdef49fea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2HldHO5xZAq",
        "outputId": "a3f1dc7d-fb97-4b12-d29f-45b24de0fde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('the', 'DT'), ('dogs', 'NNS'), ('are', 'VBP'), ('barking', 'VBG'), ('outside', 'IN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "text = \"the dogs are barking outside.\"\n",
        "word = nltk.word_tokenize(text)\n",
        "pos_tag = nltk.pos_tag(word)\n",
        "print (pos_tag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75DzNulFxea2"
      },
      "outputs": [],
      "source": [
        "# 2. Create representation of document by calculating Term Frequency and Inverse Document Frequency.\n",
        "# import required module\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqNvJX9jxg3w"
      },
      "outputs": [],
      "source": [
        "\n",
        "d0 = 'New York Times'\n",
        "d1 = 'New York Post'\n",
        "d2 = 'Los Angles Times'\n",
        "\n",
        "series = [d0, d1, d2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W06SM0bqxjTU"
      },
      "outputs": [],
      "source": [
        "# create object\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "# get tf-df values\n",
        "result = tfidf.fit_transform(series)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD_0-7SvxmX8",
        "outputId": "e4d43a87-1b2f-4bad-8301-93723d830666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Word indexes:\n",
            "{'new': 2, 'york': 5, 'times': 4, 'post': 3, 'los': 1, 'angles': 0}\n",
            "\n",
            "tf-idf value:\n",
            "  (0, 4)\t0.5773502691896257\n",
            "  (0, 5)\t0.5773502691896257\n",
            "  (0, 2)\t0.5773502691896257\n",
            "  (1, 3)\t0.680918560398684\n",
            "  (1, 5)\t0.5178561161676974\n",
            "  (1, 2)\t0.5178561161676974\n",
            "  (2, 0)\t0.6227660078332259\n",
            "  (2, 1)\t0.6227660078332259\n",
            "  (2, 4)\t0.4736296010332684\n",
            "\n",
            "tf-idf values in matrix form:\n",
            "[[0.         0.         0.57735027 0.         0.57735027 0.57735027]\n",
            " [0.         0.         0.51785612 0.68091856 0.         0.51785612]\n",
            " [0.62276601 0.62276601 0.         0.         0.4736296  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# get indexing\n",
        "print('\\nWord indexes:')\n",
        "print(tfidf.vocabulary_)\n",
        "\n",
        "# display tf-idf values\n",
        "print('\\ntf-idf value:')\n",
        "print(result)\n",
        "\n",
        "# in matrix form\n",
        "print('\\ntf-idf values in matrix form:')\n",
        "print(result.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Analytics - Theory with Viva Q&A\n",
        "\n",
        "## 1. What is Text Analytics?\n",
        "\n",
        "**Answer:**  \n",
        "Text Analytics is the process of extracting meaningful information from unstructured text data. It involves techniques that convert text into structured data, making it easier to analyze patterns, trends, and insights. Text analytics is used in applications such as sentiment analysis, document classification, keyword extraction, and topic modeling.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Why is Text Analytics important?\n",
        "\n",
        "**Answer:**  \n",
        "Text data is one of the most abundant forms of data (emails, reviews, social media posts, etc.). Text Analytics allows organizations to:  \n",
        "- Understand customer sentiment  \n",
        "- Automate document classification  \n",
        "- Extract actionable insights from large volumes of text  \n",
        "- Improve decision-making based on text data\n",
        "\n",
        "---\n",
        "\n",
        "## 3. What are common operations performed in Text Analytics?\n",
        "\n",
        "**Answer:**  \n",
        "- Tokenization (splitting text into words or sentences)  \n",
        "- Removing stopwords (common words like \"the\", \"is\", etc.)  \n",
        "- Stemming and Lemmatization (reducing words to their root form)  \n",
        "- Named Entity Recognition (identifying names, organizations, locations, etc.)  \n",
        "- Part-of-Speech Tagging (identifying nouns, verbs, adjectives, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "## 4. What is Natural Language Toolkit (NLTK)?\n",
        "\n",
        "**Answer:**  \n",
        "NLTK is a popular Python library used for performing natural language processing (NLP) and text analysis tasks. It provides easy-to-use interfaces for text preprocessing, linguistic data, and text mining operations.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. What text analysis operations can be done using NLTK?\n",
        "\n",
        "**Answer:**  \n",
        "- Tokenization  \n",
        "- Stopword removal  \n",
        "- Stemming and Lemmatization  \n",
        "- Part-of-Speech tagging  \n",
        "- Named Entity Recognition  \n",
        "- Concordance and word frequency analysis\n",
        "\n",
        "---\n",
        "\n",
        "## 6. What is TF-IDF?\n",
        "\n",
        "**Answer:**  \n",
        "TF-IDF stands for **Term Frequency - Inverse Document Frequency**. It is a statistical measure used to evaluate how important a word is in a document relative to a collection of documents (corpus).\n",
        "\n",
        "---\n",
        "\n",
        "## 7. How does TF-IDF work?\n",
        "\n",
        "**Answer:**  \n",
        "- **Term Frequency (TF)** measures how frequently a word occurs in a document.  \n",
        "- **Inverse Document Frequency (IDF)** measures how important a word is by reducing the weight of commonly used words across all documents.  \n",
        "TF-IDF gives a high score to words that are frequent in a document but rare in the corpus, making them important for that document.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Why is TF-IDF used in text analysis?\n",
        "\n",
        "**Answer:**  \n",
        "TF-IDF helps in:  \n",
        "- Extracting important keywords from documents  \n",
        "- Building document similarity models  \n",
        "- Improving the performance of text classification models  \n",
        "- Reducing the effect of common but uninformative words\n",
        "\n",
        "---\n",
        "\n",
        "## 9. What is Bag of Words (BoW)?\n",
        "\n",
        "**Answer:**  \n",
        "Bag of Words is a text representation technique where a document is represented as a collection of its words, disregarding grammar and word order but keeping track of word frequency.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. What are the key characteristics of Bag of Words?\n",
        "\n",
        "**Answer:**  \n",
        "- It represents text as a vector of word counts or word frequencies.  \n",
        "- The vocabulary is built from all unique words in the corpus.  \n",
        "- The position and context of words are ignored.  \n",
        "- It is simple to implement and works well for many tasks, but it can lead to sparse and high-dimensional feature spaces.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. How does Bag of Words differ from TF-IDF?\n",
        "\n",
        "**Answer:**  \n",
        "- **BoW** counts how many times a word appears in a document without considering its importance.  \n",
        "- **TF-IDF** adjusts word frequencies by reducing the weight of common words across documents, giving importance to rare and informative words.\n",
        "\n",
        "---\n",
        "\n",
        "## 12. What are the limitations of Bag of Words?\n",
        "\n",
        "**Answer:**  \n",
        "- It ignores the context and semantics of words.  \n",
        "- It does not capture the order of words (syntax).  \n",
        "- It can create very large and sparse matrices when applied to large corpora.\n",
        "\n"
      ],
      "metadata": {
        "id": "2-jS4O3Lvs0M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5Ook_NUx3YA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}