# Diffusion Prompt Embedder

[![PyPI version](https://img.shields.io/pypi/v/diffusion-prompt-embedder.svg)](https://pypi.org/project/diffusion-prompt-embedder/)
[![Python Version](https://img.shields.io/pypi/pyversions/diffusion-prompt-embedder.svg)](https://pypi.org/project/diffusion-prompt-embedder/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code Coverage](https://img.shields.io/badge/coverage-100%25-brightgreen.svg)](https://github.com/jannchie/diffusion-prompt-embedder)

ä¸€ä¸ªä¸“é—¨ç”¨äºè§£æå’Œå¤„ç†å¸¦æœ‰æƒé‡çš„æç¤ºæ–‡æœ¬çš„ Python åº“ï¼Œæ”¯æŒåµŒå…¥ç”Ÿæˆå’Œæ ‡è®°åŒ–ï¼Œä¸º Stable Diffusion ç­‰ AI æ¨¡å‹æä¾›å¢å¼ºçš„æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚å®ƒå…¼å®¹ SD Web UI çš„æƒé‡æç¤ºï¼Œä½†ä¸åŒ…æ‹¬è°ƒåº¦éƒ¨åˆ†ã€‚

## ç‰¹æ€§

- ğŸ’¬ **æç¤ºè§£æ**: è§£æå¸¦æœ‰æƒé‡æ ‡è®°çš„æ–‡æœ¬æç¤ºï¼ˆä¾‹å¦‚ `a (cat:1.5) in the garden`ï¼‰
- ğŸ”¢ **æƒé‡ç®¡ç†**: æ”¯æŒæ­£å‘æƒé‡ `(text)` å’Œè´Ÿå‘æƒé‡ `[text]` è¯­æ³•
- ğŸ“š **CLIP é›†æˆ**: æ— ç¼é›†æˆ CLIP æ–‡æœ¬æ¨¡å‹è¿›è¡ŒåµŒå…¥ç”Ÿæˆ
- ğŸ”„ **æ‰¹å¤„ç†æ”¯æŒ**: é«˜æ•ˆå¤„ç†å¤šä¸ªæç¤ºçš„æ‰¹å¤„ç†
- ğŸª„ **é•¿æ–‡æœ¬å¤„ç†**: æ”¯æŒè¶…å‡º CLIP æ ‡å‡†ä¸Šä¸‹æ–‡é•¿åº¦çš„é•¿æç¤º

## å®‰è£…

ä½¿ç”¨ pip å®‰è£…åŸºç¡€åº“:

```bash
pip install diffusion-prompt-embedder
```

## ä½¿ç”¨ç¤ºä¾‹

### è§£æå¸¦æœ‰æƒé‡çš„æç¤º

```python
from diffusion_prompt_embedder import parse_prompt_attention

# åŸºæœ¬è§£æ
result = parse_prompt_attention("a (cat:1.5) in the garden")
print(result)  # [['a ', 1.0], ['cat', 1.5], [' in the garden', 1.0]]

# ä½¿ç”¨æ–¹æ‹¬å·é™ä½æƒé‡
result = parse_prompt_attention("a [cat] in the garden")
print(result)  # [['a ', 1.0], ['cat', 0.9090909090909091], [' in the garden', 1.0]]

# å¤æ‚æç¤ºç¤ºä¾‹
result = parse_prompt_attention("a (((house:1.3)) [on] a (hill:0.5), sun, (((sky))).")
print(result)
```

### ç”Ÿæˆ CLIP åµŒå…¥

```python
import torch
from transformers import CLIPTokenizer, CLIPTextModel
from prompt_parser import get_embeddings_sd15

# åˆå§‹åŒ– CLIP æ¨¡å‹
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")
text_encoder = CLIPTextModel.from_pretrained(
    "openai/clip-vit-large-patch14",
    torch_dtype=torch.float16
).to("cuda")

# ç”ŸæˆåµŒå…¥
prompt_embeds, neg_prompt_embeds = get_embeddings_sd15(
    tokenizer=tokenizer,
    text_encoder=text_encoder,
    prompt="a (white:1.2) cat",
    neg_prompt="blur, bad quality",
    clip_skip=1  # å¯é€‰ï¼šè·³è¿‡ CLIP æ¨¡å‹ä¸­çš„å±‚
)

# æ‰¹å¤„ç†å¤šä¸ªæç¤º
from prompt_parser import get_embeddings_sd_15_batch

batch_embeds = get_embeddings_sd_15_batch(
    tokenizer=tokenizer,
    text_encoder=text_encoder,
    prompts=["a (white:1.2) cat", "a (blue:1.4) dog", "a red bird"]
)
```

## æç¤ºè¯­æ³•

### åŸºæœ¬æƒé‡è¯­æ³•

- `(text)` - å°†æç¤ºçš„æƒé‡æå‡ 1.1 å€
- `(text:1.5)` - å°†æç¤ºçš„æƒé‡è®¾ç½®ä¸º 1.5
- `[text]` - å°†æç¤ºçš„æƒé‡é™ä½ä¸ºåŸæ¥çš„ 1/1.1
- `\( \[ \) \]` - ä½¿ç”¨åæ–œæ è½¬ä¹‰æ‹¬å·å­—ç¬¦

### BREAK è¯­æ³•

ä½¿ç”¨ `BREAK` å…³é”®å­—åœ¨æç¤ºä¸­åˆ›å»ºæ–­ç‚¹ï¼š

```python
result = parse_prompt_attention("text1 BREAK text2")
# ç»“æœ: [["text1", 1.0], ["BREAK", -1], ["text2", 1.0]]
```

## å¼€å‘

å…‹éš†ä»“åº“å¹¶å®‰è£…å¼€å‘ä¾èµ–ï¼š

```bash
git clone https://github.com/jannchie/diffusion-prompt-parser.git
cd diffusion-prompt-parser
pip install -e ".[dev]"
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
pytest
```

## è®¸å¯è¯

[MIT](https://opensource.org/licenses/MIT)

## ä½œè€…

- Jianqi Pan ([@jannchie](https://github.com/jannchie))
