{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c272fc2f476448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T13:11:12.735701Z",
     "start_time": "2025-03-18T13:11:12.696298Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "import configparser\n",
    "\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "\n",
    "\n",
    "from fluxdataqaqc import Data, QaQc, Plot\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8941d6d6ea77b2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T13:11:13.871257Z",
     "start_time": "2025-03-18T13:11:13.865230Z"
    }
   },
   "outputs": [],
   "source": [
    "#sys.path.append(\"//\")\n",
    "sys.path.append(\"../../../micromet\")\n",
    "import micromet\n",
    "from micromet import AmerifluxDataProcessor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79870020ee682392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Transformer\n",
    "EPSG = 5070\n",
    "# load initial flux data\n",
    "station = 'US-UTW'\n",
    "config_path = f'../../station_config/{station}.ini'\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_path)\n",
    "\n",
    "#station = 'US-UTW'\n",
    "stat_configs = micromet.load_configs(station,\n",
    "                                     config_path='../../station_config/',\n",
    "                                     secrets_path=\"../../secrets/config.ini\")\n",
    "\n",
    "df = micromet.fetch_and_preprocess_data(stat_configs['url'], station, startdate='2022-01-01',)\n",
    "\n",
    "d = Data(config_path)\n",
    "d.df.index.freq = '30min'\n",
    "df = d.df.rename(columns=d.inv_map)\n",
    "q = QaQc(d, daily_frac=3/4, max_interp_hours=4, max_interp_hours_night=6)\n",
    "\n",
    "# make copies of daily results of different correction options\n",
    "q.correct_data(meth='ebr', et_gap_fill=True)\n",
    "ebr_gapfilled = q.df\n",
    "\n",
    "\n",
    "latitude = config['METADATA']['station_latitude']\n",
    "longitude = config['METADATA']['station_longitude']\n",
    "\n",
    "# Define the transformer from WGS84 to NAD83 Conus Albers\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{EPSG}\", always_xy=True)\n",
    "\n",
    "# Perform the transformation\n",
    "station_x, station_y = transformer.transform(longitude, latitude)\n",
    "\n",
    "print(f\"Projected Coordinates in NAD83 Conus Albers (EPSG:5070): X={station_x}, Y={station_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161bdf521b053bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(config['METADATA']['climate_file_path'],\n",
    "                   skiprows=int(config['METADATA']['skiprows']))\n",
    "data['datetime_start'] = pd.to_datetime(data['datetime_start'])\n",
    "data = data[(data['datetime_start'].dt.hour >= 6)&(data['datetime_start'].dt.hour <= 18)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ca886f15c1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "foot_xy = {}\n",
    "\n",
    "\n",
    "#df = pd.read_csv(\"../../station_data/US-CdM_HH_202306141100_202410081700.csv\")\n",
    "for col in data.columns:\n",
    "    if \"fetch\" in col.lower():\n",
    "        data = micromet.polar_to_cartesian_dataframe(data, wd_column='WD',dist_column=col)\n",
    "\n",
    "        data[f'X_{col}'] = data[f'X_{col}'] + station_x\n",
    "        data[f'Y_{col}'] = data[f'Y_{col}'] + station_y\n",
    "\n",
    "        foot_xy[col] = micromet.aggregate_to_daily_centroid(data,'datetime_start',f'X_{col}',f'Y_{col}',weighted=True)\n",
    "\n",
    "daily_pnts = foot_xy['FETCH_55']\n",
    "daily_pnts['geometry'] = gpd.points_from_xy(daily_pnts['X_FETCH_55'],\n",
    "                                      daily_pnts['Y_FETCH_55'])\n",
    "daily_pnts['date'] = pd.to_datetime(daily_pnts['Date'])\n",
    "daily_pnts.drop(columns='Date', inplace=True)\n",
    "daily_pnts.set_index('date', inplace=True)\n",
    "daily_pnts = pd.concat([ebr_gapfilled, daily_pnts], axis=1)\n",
    "daily_pnts_geo = gpd.GeoDataFrame(daily_pnts, geometry=daily_pnts.geometry)\n",
    "\n",
    "data['ETpos'] = np.where(data['ET']>=0,data['ET'],np.nan)\n",
    "#\n",
    "data = data.set_index('datetime_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d971bb9d5dac119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(data['X_FETCH_55'],data['Y_FETCH_55'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644b2f16cd6833f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb083c703a3898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density_raster(density, transform, gdf, show_points=True):\n",
    "    \"\"\"\n",
    "    Plots the density raster and overlays the input points.\n",
    "\n",
    "    Parameters:\n",
    "        density (numpy.ndarray): The rasterized density grid.\n",
    "        transform (Affine): Raster transform for correct alignment.\n",
    "        gdf (GeoDataFrame): The original point data to overlay.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get raster bounds\n",
    "    xmin, ymax = transform.c, transform.f  # Top-left corner\n",
    "    res = transform.a  # Cell size\n",
    "    xmax = xmin + (density.shape[1] * res)\n",
    "    ymin = ymax - (density.shape[0] * res)\n",
    "\n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot the density raster\n",
    "    extent = [xmin, xmax, ymin, ymax]  # (left, right, bottom, top)\n",
    "    im = ax.imshow(density, extent=extent, origin=\"upper\", cmap=\"viridis\", alpha=0.7)\n",
    "\n",
    "    if show_points:\n",
    "        # Overlay the input points\n",
    "        gdf.plot(ax=ax, color=\"red\", markersize=10, alpha=0.7, edgecolor=\"black\")\n",
    "\n",
    "    # Add colorbar and labels\n",
    "    cbar = plt.colorbar(im, ax=ax, label=\"Normalized Density\")\n",
    "    ax.set_title(\"Density Raster\")\n",
    "    ax.set_xlabel(\"Easting (m)\")\n",
    "    ax.set_ylabel(\"Northing (m)\")\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aacdc9fdd64f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "\n",
    "def concat_fetch_gdf(data, epsg=5070):\n",
    "    dataxy = data.dropna(subset=['X_FETCH_90',\n",
    "                               'Y_FETCH_90',\n",
    "                               'X_FETCH_55',\n",
    "                               'Y_FETCH_55',\n",
    "                               'X_FETCH_40',\n",
    "                               'Y_FETCH_40'],\n",
    "                       how='any')\n",
    "\n",
    "    dates = np.concatenate((dataxy.index.values, dataxy.index.values, dataxy.index.values))\n",
    "\n",
    "    xs = np.concatenate((dataxy['X_FETCH_90'].values, dataxy['X_FETCH_55'].values, dataxy['X_FETCH_40'].values))\n",
    "    ys = np.concatenate((dataxy['Y_FETCH_90'].values,  dataxy['Y_FETCH_55'].values,  dataxy['Y_FETCH_40'].values))\n",
    "\n",
    "    weights = np.concatenate(([90]*len(dataxy['X_FETCH_90']),\n",
    "                        [55]*len(dataxy['X_FETCH_55']),\n",
    "                        [40]*len(dataxy['X_FETCH_40'])))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({'datetime_start':dates,\n",
    "                       'x': xs,\n",
    "                       'y': ys,\n",
    "                       'weights': weights,\n",
    "                       })\n",
    "\n",
    "    df = df.set_index('datetime_start')\n",
    "\n",
    "    dfday = df.groupby(pd.Grouper(freq='1D')).apply(\n",
    "                    lambda g: pd.Series(\n",
    "                        {\n",
    "                            'x': (g['x'] * g[\"weights\"]).sum() / g[\"weights\"].sum(),\n",
    "                            'y': (g['y'] * g[\"weights\"]).sum() / g[\"weights\"].sum(),\n",
    "                            'weights': g['weights'].mean(),\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    # Convert to GeoDataFrame\n",
    "    gdf_day = gpd.GeoDataFrame(dfday, geometry=[Point(xy) for xy in zip(dfday.x, dfday.y)])\n",
    "\n",
    "    # Convert to GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=[Point(xy) for xy in zip(df.x, df.y)])\n",
    "\n",
    "    # Optionally set a CRS (e.g., WGS84)\n",
    "    gdf_day = gdf_day.set_crs(epsg=epsg)\n",
    "    gdf_day = gdf_day.dropna()\n",
    "\n",
    "    gdf = gdf.set_crs(epsg=epsg)\n",
    "    gdf = gdf.dropna()\n",
    "\n",
    "    return gdf_day, gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2927ca0636613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import rasterio.crs\n",
    "\n",
    "epsg = 5070\n",
    "\n",
    "gdf_day, gdf = concat_fetch_gdf(data,epsg=epsg)\n",
    "\n",
    "res = 100\n",
    "\n",
    "# List to hold KDE results for each day\n",
    "kde_results = []\n",
    "\n",
    "xmin = gdf.x.min()\n",
    "xmax = gdf.x.max()\n",
    "ymin = gdf.y.min()\n",
    "ymax = gdf.y.max()\n",
    "\n",
    "# Step 1: Create 2D grid\n",
    "xvals = np.linspace(xmin, xmax, res)  # X range\n",
    "yvals = np.linspace(ymin, ymax, res)  # Y range\n",
    "\n",
    "X, Y = np.meshgrid(xvals, yvals)\n",
    "positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "\n",
    "for day in gdf_day.index.date:\n",
    "    hf_hr = gdf[gdf.index.date == day]\n",
    "    m1 = hf_hr['x']\n",
    "    m2 = hf_hr['y']\n",
    "\n",
    "    values = np.vstack([m1, m2])\n",
    "    kernel = stats.gaussian_kde(values, weights=hf_hr['weights'])\n",
    "    Z = np.reshape(kernel(positions).T, X.shape)\n",
    "\n",
    "    # Step 3: Compute 90th percentile threshold\n",
    "    threshold_90 = np.percentile(Z, 90)\n",
    "\n",
    "    # Step 4: Apply mask (set values > 90th percentile to NaN)\n",
    "    Z_masked = np.where(Z > threshold_90, Z, np.nan)\n",
    "\n",
    "    # Step 5: Normalize the masked values\n",
    "    valid_sum = np.nansum(Z_masked)  # Sum only valid (non-NaN) values\n",
    "    if valid_sum > 0:\n",
    "        Z_normalized = Z_masked / valid_sum  # Normalize\n",
    "    else:\n",
    "        Z_normalized = Z_masked  # Avoid division by zero\n",
    "\n",
    "    # Step 6: Create an xarray DataArray with georeferencing\n",
    "    kde_da = xr.DataArray(\n",
    "        Z_masked[:, :, np.newaxis],\n",
    "        coords={'X': xvals,\n",
    "                'Y': yvals,\n",
    "                'time': [day],\n",
    "                },  # Adjust for your coordinate system\n",
    "        dims=['Y', 'X', 'time'],\n",
    "        name='KDE',\n",
    "    )\n",
    "\n",
    "    # Optional: Assign CRS (WGS 84 for example)\n",
    "\n",
    "    kde_da.attrs['crs'] = rasterio.crs.CRS.from_epsg(epsg)\n",
    "\n",
    "    # Append the result to the list\n",
    "    kde_results.append(kde_da)\n",
    "\n",
    "# Concatenate all the daily KDE results along a new 'time' dimension\n",
    "kde_da_all = xr.concat(kde_results, dim='time')\n",
    "\n",
    "# Now 'kde_da_all' contains KDE results for each day, with 'time' as the new dimension\n",
    "print(kde_da_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ac2707a854360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "kde_da_all.sel(time=datetime.date(2021, 8, 30)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e91abb330bb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Example: List of dates (could be strings or datetime objects)\n",
    "dates = ['2025-02-01', '2025-02-02', '2025-02-03']\n",
    "\n",
    "# Example: Generate some random data points for each day (replace with actual data)\n",
    "# For simplicity, we'll use random data; in your case, you would load the actual points for each day.\n",
    "data_per_day = [np.random.randn(2, 1000) for _ in dates]\n",
    "\n",
    "# Example grid for KDE (same grid for each day)\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = np.linspace(-10, 10, 100)\n",
    "grid_x, grid_y = np.meshgrid(x, y)\n",
    "\n",
    "# List to hold KDE results for each day\n",
    "kde_results = []\n",
    "\n",
    "# Loop over each day's data\n",
    "for day, data in zip(dates, data_per_day):\n",
    "    # Fit the KDE to the data for the day\n",
    "    kde = gaussian_kde(data)\n",
    "    grid_points = np.vstack([grid_x.ravel(), grid_y.ravel()])\n",
    "    kde_values = kde(grid_points).reshape(grid_x.shape)\n",
    "\n",
    "    # Create a DataArray for the day and add it to the list\n",
    "    kde_da_day = xr.DataArray(\n",
    "        kde_values,\n",
    "        coords=[('latitude', y), ('longitude', x), ],\n",
    "        dims=['latitude', 'longitude'],\n",
    "        name='KDE',\n",
    "        attrs={'time': day}  # Add the time (or date) as an attribute\n",
    "    )\n",
    "\n",
    "    # Append the result to the list\n",
    "    kde_results.append(kde_da_day)\n",
    "\n",
    "# Concatenate all the daily KDE results along a new 'time' dimension\n",
    "kde_da_all = xr.concat(kde_results, dim='time')\n",
    "\n",
    "# Now 'kde_da_all' contains KDE results for each day, with 'time' as the new dimension\n",
    "print(kde_da_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5d1a6eec020c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.dropna().plot()#.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5592e5f0d28a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "m1 = gdf['x']\n",
    "m2 = gdf['y']\n",
    "\n",
    "xmin = m1.min()\n",
    "xmax = m1.max()\n",
    "ymin = m2.min()\n",
    "ymax = m2.max()\n",
    "\n",
    "X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "values = np.vstack([m1, m2])\n",
    "kernel = stats.gaussian_kde(values)\n",
    "Z = np.reshape(kernel(positions).T, X.shape)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n",
    "          extent=[xmin, xmax, ymin, ymax])\n",
    "ax.plot(m1, m2, 'k.', markersize=2)\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([ymin, ymax])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab445f0a5f679bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n",
    "          extent=[xmin, xmax, ymin, ymax])\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([ymin, ymax])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dcc2aad32acc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "data = data.dropna(subset=['X_FETCH_90',\n",
    "                           'Y_FETCH_90',\n",
    "                           'X_FETCH_55',\n",
    "                           'Y_FETCH_55',\n",
    "                           'X_FETCH_40',\n",
    "                           'Y_FETCH_40'],\n",
    "                   how='any')\n",
    "\n",
    "xs = np.concatenate((data['X_FETCH_90'].values, data['X_FETCH_55'].values, data['X_FETCH_40'].values))\n",
    "ys = np.concatenate((data['Y_FETCH_90'].values,  data['Y_FETCH_55'].values,  data['Y_FETCH_40'].values))\n",
    "weights = np.concatenate(([90]*len(data['X_FETCH_90']),\n",
    "                    [55]*len(data['X_FETCH_55']),\n",
    "                    [40]*len(data['X_FETCH_40'])))\n",
    "\n",
    "\n",
    "#buffer_distance = 0\n",
    "resolution = 50\n",
    "\n",
    "# Define raster extent with buffer\n",
    "xmin = np.min(xs)\n",
    "ymin = np.min(ys)\n",
    "xmax = np.max(xs)\n",
    "ymax = np.max(ys)\n",
    "\n",
    "# Create a mesh grid\n",
    "xgrid = np.arange(xmin, xmax, resolution)\n",
    "ygrid = np.arange(ymin, ymax, resolution)\n",
    "xmesh, ymesh = np.meshgrid(xgrid, ygrid)\n",
    "\n",
    "# Perform KDE with weights\n",
    "kde = gaussian_kde(np.vstack([xs, ys]), weights=weights)\n",
    "density = kde(np.vstack([xmesh.ravel(), ymesh.ravel()])).reshape(xmesh.shape)\n",
    "\n",
    "# Normalize to ensure sum of cell values is 1\n",
    "#print(np.sum(density))\n",
    "#density /= np.sum(density)\n",
    "\n",
    "# Define raster transform\n",
    "transform = from_origin(xmin, ymax, resolution, resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf69c901a67f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74ba98781d06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data['geometry'] = gpd.points_from_xy(data['X_FETCH_55'],\n",
    "                                      data['Y_FETCH_55'])\n",
    "\n",
    "data_geo = gpd.GeoDataFrame(data, geometry=data.geometry, crs='EPSG:5070')\n",
    "data_geo = data_geo.dropna(subset=['geometry','ETimp','X_FETCH_55','Y_FETCH_55'],how='any')\n",
    "\n",
    "densitynorm = density / np.sum(density)\n",
    "\n",
    "# Get raster bounds\n",
    "xmin, ymax = transform.c, transform.f  # Top-left corner\n",
    "res = transform.a  # Cell size\n",
    "xmax = xmin + (density.shape[1] * res)\n",
    "ymin = ymax - (density.shape[0] * res)\n",
    "\n",
    "# Create the figure\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the density raster\n",
    "extent = [xmin, xmax, ymin, ymax]  # (left, right, bottom, top)\n",
    "im = ax.imshow(densitynorm, extent=extent, origin=\"upper\", cmap=\"viridis\", alpha=0.7)\n",
    "data_geo.plot(ax=ax, color=\"red\", markersize=10, alpha=0.3, edgecolor=\"black\")\n",
    "print(extent)\n",
    "\n",
    "# Add colorbar and labels\n",
    "cbar = plt.colorbar(im, ax=ax, label=\"Normalized Density\")\n",
    "ax.set_title(\"Density Raster\")\n",
    "ax.set_xlabel(\"Easting (m)\")\n",
    "ax.set_ylabel(\"Northing (m)\")\n",
    "\n",
    "\n",
    "ax.scatter(station_x, station_y,color='red')\n",
    "plt.xlim(xmin+5500, xmax-5500)\n",
    "plt.ylim(ymin+6500, ymax-6500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c97956d58602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def impute_evapotranspiration(df, in_field='ET', out_field='ET'):\n",
    "    \"\"\"\n",
    "    Impute missing data in a half-hourly evapotranspiration time series.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with a datetime index and a column 'ET' containing evapotranspiration data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with missing values imputed.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Avoid modifying the original DataFrame\n",
    "\n",
    "    # Ensure datetime index\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # Step 1: Linear interpolation for small gaps\n",
    "    df[out_field] = df[in_field].interpolate(method='linear', limit=4)  # Limit to prevent long-term bias\n",
    "\n",
    "    # Step 2: Seasonal and daily imputation\n",
    "    df['hour'] = df.index.hour\n",
    "    df['minute'] = df.index.minute\n",
    "    df['day_of_year'] = df.index.dayofyear\n",
    "\n",
    "    # Compute typical ET values at the same time across different years\n",
    "    daily_medians = df.groupby(['day_of_year', 'hour', 'minute'])[out_field].median()\n",
    "\n",
    "    # Impute missing values using seasonal daily median\n",
    "    def impute_from_medians(row):\n",
    "        if pd.isna(row[out_field]):\n",
    "            return daily_medians.get((row['day_of_year'], row['hour'], row['minute']), np.nan)\n",
    "        return row[out_field]\n",
    "\n",
    "    df[out_field] = df.apply(impute_from_medians, axis=1)\n",
    "\n",
    "    # Step 3: Rolling mean smoothing to refine imputations\n",
    "    df[out_field] = df[out_field].bfill().ffill()  # Handle any remaining NaNs\n",
    "    df[out_field] = df[out_field].rolling(window=6, min_periods=1, center=True).mean()  # Smooth over 3 hours\n",
    "\n",
    "    # Drop helper columns\n",
    "    df.drop(columns=['hour', 'minute', 'day_of_year'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_csv('evapotranspiration_data.csv', parse_dates=['datetime'], index_col='datetime')\n",
    "# df = impute_evapotranspiration(df)\n",
    "\n",
    "# df = pd.read_csv('evapotranspiration_data.csv', parse_dates=['datetime'], index_col='datetime')\n",
    "# imputer = EvapotranspirationImputer(df)\n",
    "# df_imputed = imputer.impute(method=\"seasonal_median\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ebc5c676caa091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('evapotranspiration_data.csv', parse_dates=['datetime'], index_col='datetime')\n",
    "data = impute_evapotranspiration(data,in_field='ETpos',out_field='ETimp')\n",
    "#df_imputed = imputer.impute(method=\"seasonal_median\")\n",
    "data['ETimp'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68a1d6c6c3484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "\n",
    "def generate_density_raster(\n",
    "    gdf,\n",
    "    resolution=50,  # Cell size in meters\n",
    "    buffer_distance=200,  # Buffer beyond extent in meters\n",
    "    epsg=5070,  # Default coordinate system\n",
    "    weight_field=\"ET\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a density raster from a point GeoDataFrame, weighted by the ET field.\n",
    "\n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): Input point GeoDataFrame with an 'ET' field.\n",
    "        resolution (float): Raster cell size in meters (default: 50m).\n",
    "        buffer_distance (float): Buffer beyond point extent (default: 200m).\n",
    "        epsg (int): Coordinate system EPSG code (default: 5070).\n",
    "        weight_field (str): Weight field name (default: ET).\n",
    "\n",
    "    Returns:\n",
    "        raster (numpy.ndarray): Normalized density raster.\n",
    "        transform (Affine): Affine transformation for georeferencing.\n",
    "        bounds (tuple): (xmin, ymin, xmax, ymax) of the raster extent.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure correct CRS\n",
    "    gdf = gdf.to_crs(epsg=epsg)\n",
    "\n",
    "    # Extract point coordinates and ET values\n",
    "    x = gdf.geometry.x\n",
    "    y = gdf.geometry.y\n",
    "    weights = gdf[weight_field].values\n",
    "\n",
    "    # Define raster extent with buffer\n",
    "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "    xmin, xmax = xmin - buffer_distance, xmax + buffer_distance\n",
    "    ymin, ymax = ymin - buffer_distance, ymax + buffer_distance\n",
    "\n",
    "    # Create a mesh grid\n",
    "    xgrid = np.arange(xmin, xmax, resolution)\n",
    "    ygrid = np.arange(ymin, ymax, resolution)\n",
    "    xmesh, ymesh = np.meshgrid(xgrid, ygrid)\n",
    "\n",
    "    # Perform KDE with weights\n",
    "    kde = gaussian_kde(np.vstack([x, y]), weights=weights)\n",
    "    density = kde(np.vstack([xmesh.ravel(), ymesh.ravel()])).reshape(xmesh.shape)\n",
    "\n",
    "    # Normalize to ensure sum of cell values is 1\n",
    "    print(np.sum(density))\n",
    "    density /= np.sum(density)\n",
    "\n",
    "    # Define raster transform\n",
    "    transform = from_origin(xmin, ymax, resolution, resolution)\n",
    "\n",
    "    return density, transform, (xmin, ymin, xmax, ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f675c7c05145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "data['geometry'] = gpd.points_from_xy(data['X_FETCH_55'],\n",
    "                                      data['Y_FETCH_55'])\n",
    "\n",
    "data_geo = gpd.GeoDataFrame(data, geometry=data.geometry, crs='EPSG:5070')\n",
    "data_geo = data_geo.dropna(subset=['geometry','ETimp','X_FETCH_55','Y_FETCH_55'],how='any')\n",
    "\n",
    "density, transform, (xmin, ymin, xmax, ymax) = generate_density_raster(data_geo,\n",
    "                                                                                resolution=10,\n",
    "                                                                                buffer_distance=10,\n",
    "                                                                                weight_field='ETimp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6e1350a7165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_density_raster(density,transform,data_geo,show_points=False)\n",
    "ax.scatter(station_x, station_y,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e003d3608a9b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9c3deeacac4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113360e46f81214",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f88e4ec8ab4b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pnts.dropna(subset=['X_FETCH_55', 'ET'], inplace=True)\n",
    "daily_pnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2329ddf414386d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd8848e7710b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformer from WGS84 to NAD83 Conus Albers\n",
    "transformer = Transformer.from_crs(\"EPSG:5070\", f\"EPSG:4326\", always_xy=True)\n",
    "\n",
    "for ind in daily_pnts.index:\n",
    "\n",
    "    # Perform the transformation\n",
    "    longitude, latitude = transformer.transform(f\"{daily_pnts.loc[ind, 'X_FETCH_55']}\",\n",
    "                                                 f\"{daily_pnts.loc[ind, 'Y_FETCH_55']}\")\n",
    "\n",
    "    print(longitude, latitude)\n",
    "    #(EPSG:5070)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543726649961f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "keyconf_path = f'../../secrets/config.ini'\n",
    "keyconf = configparser.ConfigParser()\n",
    "keyconf.read(keyconf_path)\n",
    "\n",
    "# set your API key before making the request\n",
    "header = {\"Authorization\": keyconf['OPENET']['key']}\n",
    "\n",
    "# Define the transformer from WGS84 to NAD83 Conus Albers\n",
    "transformer = Transformer.from_crs(\"EPSG:5070\", f\"EPSG:4326\", always_xy=True)\n",
    "\n",
    "\n",
    "oet = {}\n",
    "i = 0\n",
    "for ind in daily_pnts.index[:]:\n",
    "\n",
    "    # Perform the transformation\n",
    "    longitude, latitude = transformer.transform(f\"{daily_pnts.loc[ind, 'X_FETCH_55']}\",\n",
    "                                                 f\"{daily_pnts.loc[ind, 'Y_FETCH_55']}\")\n",
    "\n",
    "    print(longitude, latitude)\n",
    "\n",
    "\n",
    "    args = {\n",
    "      \"date_range\": [\n",
    "        f\"{ind:%Y-%m-%d}\",\n",
    "        f\"{ind+pd.Timedelta(days=1):%Y-%m-%d}\"\n",
    "      ],\n",
    "      \"file_format\": \"JSON\",\n",
    "      \"geometry\": [\n",
    "        -121.36322,\n",
    "        38.87626\n",
    "      ],\n",
    "      \"interval\": \"daily\",\n",
    "      \"model\": \"Ensemble\",\n",
    "      \"reference_et\": \"gridMET\",\n",
    "      \"units\": \"mm\",\n",
    "      \"variable\": \"ET\"\n",
    "    }\n",
    "\n",
    "    # query the api\n",
    "    resp = requests.post(\n",
    "        headers=header,\n",
    "        json=args,\n",
    "        url=\"https://openet-api.org/raster/timeseries/point\"\n",
    "    )\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        pass\n",
    "    else:\n",
    "        print(resp.json(),i)\n",
    "        oet[ind] = resp.json()[0]['et']\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f511d51fdfa8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read('../../secrets/config.ini')\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "host = config['DEFAULT']['ip']\n",
    "pw = config['DEFAULT']['pw']\n",
    "user = config['DEFAULT']['login']\n",
    "\n",
    "encoded_password = urllib.parse.quote_plus(pw)\n",
    "\n",
    "def postconn_et(encoded_password, host='localhost',user='postgres',port='5432',db='groundwater', schema = 'groundwater'):\n",
    "    connection_text = \"postgresql+psycopg2://{:}:{:}@{:}:{:}/{:}?gssencmode=disable\".format(user,encoded_password,host,port,db)\n",
    "    return create_engine(connection_text, connect_args={'options': '-csearch_path={}'.format(schema)})\n",
    "\n",
    "\n",
    "engine = postconn_et(encoded_password, host=host, user=user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fdbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "import numpy as np\n",
    "def addapt_numpy_float64(numpy_float64):\n",
    "    return AsIs(numpy_float64)\n",
    "def addapt_numpy_int64(numpy_int64):\n",
    "    return AsIs(numpy_int64)\n",
    "register_adapter(np.float64, addapt_numpy_float64)\n",
    "register_adapter(np.int64, addapt_numpy_int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce8c98f9ab6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pnts['openet_ens_mm'] = daily_pnts.index.map(oet)\n",
    "daily_pnts['stationid'] = 'US-UTM'\n",
    "daily_pnts_no_geo = daily_pnts.drop(['geometry'], axis=1)\n",
    "daily_pnts_no_geo.to_parquet(\"G:/Shared drives/UGS_Flux/Data_Processing/Wellington/daily_footprint_pnts_utm.parquet\")\n",
    "daily_pnts_no_geo.to_sql('daily_et',engine,if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a866bc8cfc0f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pnts_geo.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c9fc1d492f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pnts = pd.read_parquet(\"G:/Shared drives/UGS_Flux/Data_Processing/Wellington/daily_footprint_pnts_utm.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b44cb2dd94d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pnts['openet_ens_mm'] = daily_pnts.index.map(oet)\n",
    "daily_pnts['stationid'] = 'US-UTW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534bcf62528a71a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(daily_pnts['ET_fill'],daily_pnts['openet_ens_mm'])\n",
    "xline = np.linspace(0,18,100)\n",
    "plt.plot(xline,xline)\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab24333d20da6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(daily_pnts['ET_corr'],daily_pnts['openet_ens_mm'])\n",
    "xline = np.linspace(0,18,100)\n",
    "plt.plot(xline,xline)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb70cd60a108a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(daily_pnts['ET'],daily_pnts['openet_ens_mm'])\n",
    "xline = np.linspace(0,18,100)\n",
    "plt.plot(xline,xline)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00191cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(daily_pnts.index,daily_pnts['ET'],label='Measured ET')\n",
    "plt.plot(daily_pnts.index,daily_pnts['openet_ens_mm'],label='OpenET ET')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d16a94de8a946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def generate_density_raster(\n",
    "    gdf,\n",
    "    resolution=50,  # Cell size in meters\n",
    "    buffer_distance=200,  # Buffer beyond extent in meters\n",
    "    epsg=5070  # Default coordinate system\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a density raster from a point GeoDataFrame, weighted by the ET field.\n",
    "\n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): Input point GeoDataFrame with an 'ET' field.\n",
    "        resolution (float): Raster cell size in meters (default: 50m).\n",
    "        buffer_distance (float): Buffer beyond point extent (default: 200m).\n",
    "        epsg (int): Coordinate system EPSG code (default: 5070).\n",
    "\n",
    "    Returns:\n",
    "        raster (numpy.ndarray): Normalized density raster.\n",
    "        transform (Affine): Affine transformation for georeferencing.\n",
    "        bounds (tuple): (xmin, ymin, xmax, ymax) of the raster extent.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure correct CRS\n",
    "    gdf = gdf.to_crs(epsg=epsg)\n",
    "\n",
    "    # Extract point coordinates and ET values\n",
    "    x = gdf.geometry.x\n",
    "    y = gdf.geometry.y\n",
    "    weights = gdf[\"ET\"].values\n",
    "\n",
    "    # Define raster extent with buffer\n",
    "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "    xmin, xmax = xmin - buffer_distance, xmax + buffer_distance\n",
    "    ymin, ymax = ymin - buffer_distance, ymax + buffer_distance\n",
    "\n",
    "    # Create a mesh grid\n",
    "    xgrid = np.arange(xmin, xmax, resolution)\n",
    "    ygrid = np.arange(ymin, ymax, resolution)\n",
    "    xmesh, ymesh = np.meshgrid(xgrid, ygrid)\n",
    "\n",
    "    # Perform KDE with weights\n",
    "    kde = gaussian_kde(np.vstack([x, y]), weights=weights)\n",
    "    density = kde(np.vstack([xmesh.ravel(), ymesh.ravel()])).reshape(xmesh.shape)\n",
    "\n",
    "    # Normalize to ensure sum of cell values is 1\n",
    "    density /= np.sum(density)\n",
    "\n",
    "    # Define raster transform\n",
    "    transform = from_origin(xmin, ymax, resolution, resolution)\n",
    "\n",
    "    return density, transform, (xmin, ymin, xmax, ymax)\n",
    "\n",
    "# Example usage:\n",
    "# gdf = gpd.read_file(\"points.shp\")\n",
    "# density_raster, transform, bounds = generate_density_raster(gdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b2751658256fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://services1.arcgis.com/99lidPhWCzftIe9K/arcgis/rest/services/WaterRelatedLandUse/FeatureServer/0/query?where=1%3D1&outFields=*&geometry=-110.75%2C39.438%2C-110.710%2C39.45&geometryType=esriGeometryEnvelope&inSR=4326&spatialRel=esriSpatialRelIntersects&outSR=5070&f=json\"\n",
    "\n",
    "well_fields = gpd.read_file(url)\n",
    "well_fields = well_fields[well_fields['OBJECTID'].isin([53384,57425])]\n",
    "well_fields.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787649107ac1771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the extent of the shapefile\n",
    "total_bounds = well_fields.total_bounds\n",
    "\n",
    "# Get minX, minY, maxX, maxY\n",
    "minX, minY, maxX, maxY = total_bounds\n",
    "\n",
    "# Create a fishnet\n",
    "x, y = (minX, minY)\n",
    "geom_array = []\n",
    "\n",
    "# Polygon Size\n",
    "square_size = 20\n",
    "while y <= maxY:\n",
    "    while x <= maxX:\n",
    "        geom = shapely.geometry.Polygon([(x,y), (x, y+square_size), (x+square_size, y+square_size), (x+square_size, y), (x, y)])\n",
    "        geom_array.append(geom)\n",
    "        x += square_size\n",
    "    x = minX\n",
    "    y += square_size\n",
    "\n",
    "fishnet = gpd.GeoDataFrame(geom_array, columns=['geometry']).set_crs('EPSG:5070')\n",
    "#fishnet.to_file('fishnet_grid.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecaf3002a69edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a geometry column\n",
    "df['geometry'] = gpd.points_from_xy(df['X_FETCH_55'], df['Y_FETCH_55'])\n",
    "df['geometry'].unique()\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "#gdf = gpd.GeoDataFrame(df, geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d92450c3e40d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
