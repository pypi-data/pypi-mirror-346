"""Module for chunking source code files using LODGenerator."""

import logging
from collections.abc import Generator
from pathlib import Path
from typing import TYPE_CHECKING, Any, TypedDict

# Use LODGenerator instead of directly using TreeSitterAnalyzer
from codemap.processor.lod import LODEntity, LODGenerator, LODLevel
from codemap.processor.tree_sitter.base import EntityType
from codemap.utils.file_utils import read_file_content

if TYPE_CHECKING:
	from codemap.config import ConfigLoader

logger = logging.getLogger(__name__)


class ChunkMetadata(TypedDict):
	"""Metadata associated with a code chunk."""

	chunk_id: str
	file_path: str
	start_line: int
	end_line: int
	entity_type: str  # Name of the EntityType enum
	entity_name: str | None
	language: str | None
	git_hash: str | None
	hierarchy_path: str  # Full path in the code hierarchy


class CodeChunk(TypedDict):
	"""Represents a chunk of code with its metadata."""

	content: str
	metadata: ChunkMetadata


class TreeSitterChunker:
	"""Chunks code files based on LODEntity structure generated by LODGenerator."""

	def __init__(self, lod_generator: LODGenerator | None = None, config_loader: "ConfigLoader | None" = None) -> None:
		"""
		Initialize the chunker.

		Args:
		    lod_generator: An instance of LODGenerator. If None, creates a new one.
		    config_loader: Configuration loader instance.

		"""
		self.lod_generator = lod_generator or LODGenerator()
		if config_loader:
			self.config_loader = config_loader
		else:
			from codemap.config import ConfigLoader

			self.config_loader = ConfigLoader()

		# Load configuration values
		embedding_config = self.config_loader.get.embedding
		chunking_config = embedding_config.chunking

		# Set constants from config with fallbacks
		self.max_hierarchy_depth = chunking_config.max_hierarchy_depth
		self.max_file_lines = chunking_config.max_file_lines

	def _get_entity_code_content(self, entity: LODEntity, file_lines: list[str]) -> str | None:
		"""Extract the raw code content for an entity using its line numbers."""
		if entity.start_line is None or entity.end_line is None:
			return None

		start_idx = entity.start_line - 1
		end_idx = entity.end_line
		if 0 <= start_idx < end_idx <= len(file_lines):
			return "\n".join(file_lines[start_idx:end_idx])
		logger.warning(
			"Invalid line numbers for entity %s in %s: start=%s, end=%s, total_lines=%d",
			entity.name,
			entity.metadata.get("file_path"),
			entity.start_line,
			entity.end_line,
			len(file_lines),
		)
		return None

	def _build_hierarchy_path(self, entity: LODEntity, parent_path: str = "") -> str:
		"""
		Build a hierarchical path string representing the entity's position in the code.

		Args:
		        entity: The current entity
		        parent_path: Path of parent entities

		Returns:
		        String representation of the hierarchy path

		"""
		entity_name = entity.name or f"<{entity.entity_type.name.lower()}>"
		if not parent_path:
			return entity_name
		return f"{parent_path}.{entity_name}"

	def _extract_nested_entities(self, entity: LODEntity) -> list[dict[str, Any]]:
		"""
		Extract information about nested entities to enhance chunk context.

		Args:
		        entity: The current entity

		Returns:
		        List of dictionaries containing info about nested entities

		"""
		nested_info = []

		def process_nested(nested_entity: LODEntity, depth: int = 1) -> None:
			"""Process a nested entity and its children recursively to extract information.

			Args:
				nested_entity: The nested entity to process
				depth: Current depth in the hierarchy (default: 1)

			Returns:
				None: Modifies nested_info in place by appending entity information
			"""
			# Skip UNKNOWN entities
			if nested_entity.entity_type == EntityType.UNKNOWN:
				return

			entity_info = {
				"type": nested_entity.entity_type.name,
				"name": nested_entity.name or f"<{nested_entity.entity_type.name.lower()}>",
				"signature": nested_entity.signature or "",
				"depth": depth,
				"line_range": f"{nested_entity.start_line}-{nested_entity.end_line}"
				if nested_entity.start_line and nested_entity.end_line
				else "",
			}
			nested_info.append(entity_info)

			# Process children (limited by configured max hierarchy depth)
			if depth < self.max_hierarchy_depth:
				for child in nested_entity.children:
					process_nested(child, depth + 1)

		# Process all direct children
		for child in entity.children:
			process_nested(child)

		return nested_info

	def _chunk_entity_recursive(
		self,
		entity: LODEntity,
		file_path: Path,
		file_lines: list[str],
		git_hash: str | None,
		language: str,
		parent_hierarchy: str = "",
		file_entity: LODEntity | None = None,
	) -> Generator[CodeChunk, None, None]:
		"""Recursive helper to generate chunks from the LODEntity tree with hierarchy context."""
		# Decide which entity types are significant enough to become their own chunk
		primary_chunkable_types = (
			EntityType.MODULE,
			EntityType.CLASS,
			EntityType.INTERFACE,
			EntityType.STRUCT,
		)

		secondary_chunkable_types = (
			EntityType.FUNCTION,
			EntityType.METHOD,
		)

		# Skip UNKNOWN entities entirely
		if entity.entity_type == EntityType.UNKNOWN:
			return

		# Build hierarchy path for this entity
		entity_hierarchy = self._build_hierarchy_path(entity, parent_hierarchy)

		# For primary entities (modules, classes), create full chunks with all their content
		if (
			entity.entity_type in primary_chunkable_types
			and entity.start_line is not None
			and entity.end_line is not None
		):
			try:
				# Get full content including all nested entities
				code_content = self._get_entity_code_content(entity, file_lines)
				if code_content:
					# Extract information about nested entities to enhance context
					nested_entities = self._extract_nested_entities(entity)

					# Construct rich chunk content with nested entity information
					content_parts = []
					content_parts.append(f"Type: {entity.entity_type.name}")
					content_parts.append(f"Path: {entity_hierarchy}")
					if entity.name:
						content_parts.append(f"Name: {entity.name}")
					if entity.signature:
						content_parts.append(f"Signature: {entity.signature}")
					if entity.docstring:
						content_parts.append(f"Docstring:\n{entity.docstring}")

					# Add structure overview
					if nested_entities:
						content_parts.append("Contains:")
						for ne in nested_entities:
							indent = "  " * ne["depth"]
							content_parts.append(
								f"{indent}- {ne['type']}: {ne['name']} {ne['signature']} (lines {ne['line_range']})"
							)

					# Add the full code
					content_parts.append(f"Code:\n```{language}\n{code_content}\n```")

					# Add raw unformatted code at the end
					content_parts.append(f"Raw:\n{code_content}")

					chunk_content = "\n\n".join(content_parts)

					# Reverted path logic: use original file_path
					# Removed relative path calculation
					chunk_id = f"{file_path!s}:{entity.start_line}-{entity.end_line}"
					metadata: ChunkMetadata = {
						"chunk_id": chunk_id,
						# Store original (likely absolute) path
						"file_path": str(file_path),
						"start_line": entity.start_line,
						"end_line": entity.end_line,
						"entity_type": entity.entity_type.name,
						"entity_name": entity.name,
						"language": language,
						"git_hash": git_hash,
						"hierarchy_path": entity_hierarchy,
					}
					yield {"content": chunk_content, "metadata": metadata}

			except (ValueError, TypeError, KeyError, AttributeError):
				logger.exception("Error processing LOD entity %s in %s", entity.name, file_path)

		# For secondary entities (functions, methods), create individual chunks
		elif (
			entity.entity_type in secondary_chunkable_types
			and entity.start_line is not None
			and entity.end_line is not None
		):
			try:
				code_content = self._get_entity_code_content(entity, file_lines)
				if code_content:
					# Use file entity if available (for better context)
					file_context = ""
					if file_entity and file_entity.entity_type == EntityType.MODULE:
						file_context = f"File: {file_entity.name or Path(str(file_path)).name}\n"

					# Construct rich chunk content
					content_parts = []
					content_parts.append(f"{file_context}Type: {entity.entity_type.name}")
					content_parts.append(f"Path: {entity_hierarchy}")
					if entity.name:
						content_parts.append(f"Name: {entity.name}")
					if entity.signature:
						content_parts.append(f"Signature: {entity.signature}")
					if entity.docstring:
						content_parts.append(f"Docstring:\n{entity.docstring}")

					# Add code with any dependencies visible in comments
					content_parts.append(f"Code:\n```{language}\n{code_content}\n```")

					# Add raw unformatted code at the end
					content_parts.append(f"Raw:\n{code_content}")

					chunk_content = "\n\n".join(content_parts)

					# Reverted path logic: use original file_path
					# Removed relative path calculation
					chunk_id = f"{file_path!s}:{entity.start_line}-{entity.end_line}"
					metadata: ChunkMetadata = {
						"chunk_id": chunk_id,
						# Store original (likely absolute) path
						"file_path": str(file_path),
						"start_line": entity.start_line,
						"end_line": entity.end_line,
						"entity_type": entity.entity_type.name,
						"entity_name": entity.name,
						"language": language,
						"git_hash": git_hash,
						"hierarchy_path": entity_hierarchy,
					}
					yield {"content": chunk_content, "metadata": metadata}

			except (ValueError, TypeError, KeyError, AttributeError):
				logger.exception("Error processing LOD entity %s in %s", entity.name, file_path)

		# Recursively process children, remove repo_path pass
		for child in entity.children:
			yield from self._chunk_entity_recursive(
				child,
				file_path,
				file_lines,
				git_hash,
				language,
				entity_hierarchy,
				file_entity=file_entity,
			)

	def chunk_file(
		self,
		file_path: Path,
		git_hash: str | None = None,
		lod_level: LODLevel = LODLevel.FULL,  # Use FULL for max info, not DETAIL
	) -> Generator[CodeChunk, None, None]:
		"""
		Generates code chunks for a given file using LODGenerator.

		Args:
		    file_path: The path to the file to chunk.
		    git_hash: Optional Git hash of the file content.
		    lod_level: The level of detail to request from LODGenerator.

		Yields:
		    CodeChunk dictionaries, each representing a semantically rich code chunk.

		"""
		try:
			content = read_file_content(file_path)
			if content is None:
				logger.debug("Skipping file %s - file does not exist or cannot be read", file_path)
				return

			# Generate the LODEntity tree for the file using the specified level of detail
			root_entity = self.lod_generator.generate_lod(file_path, lod_level)

			if not root_entity:
				logger.debug("LODGenerator returned no entity for %s, skipping chunking", file_path)
				return

			# Language should be available in the root entity metadata now
			resolved_language = root_entity.metadata.get("language", "unknown")
			file_lines = content.splitlines()

			# First, create a chunk for the entire file if it's small enough
			if len(file_lines) < self.max_file_lines:
				# Create a chunk for the entire file
				try:
					whole_file_content = "\n".join(file_lines)

					# Information about the file as a whole
					content_parts = []
					content_parts.append("Type: FILE")
					file_name = Path(file_path).name
					content_parts.append(f"Path: {file_name}")
					content_parts.append(f"Name: {file_name}")

					# Add docstring if the file has one (module docstring)
					if root_entity.docstring:
						content_parts.append(f"Docstring:\n{root_entity.docstring}")

					# Get structure overview
					nested_entities = self._extract_nested_entities(root_entity)
					if nested_entities:
						content_parts.append("Contains:")
						for ne in nested_entities:
							indent = "  " * ne["depth"]
							content_parts.append(
								f"{indent}- {ne['type']}: {ne['name']} {ne['signature']} (lines {ne['line_range']})"
							)

					# Add the full code
					content_parts.append(f"Code:\n```{resolved_language}\n{whole_file_content}\n```")

					# Add raw unformatted code at the end
					content_parts.append(f"Raw:\n{whole_file_content}")

					chunk_content = "\n\n".join(content_parts)

					chunk_id = f"{file_path!s}:1-{len(file_lines)}"
					metadata: ChunkMetadata = {
						"chunk_id": chunk_id,
						"file_path": str(file_path),
						"start_line": 1,
						"end_line": len(file_lines),
						"entity_type": "FILE",
						"entity_name": file_name,
						"language": resolved_language,
						"git_hash": git_hash,
						"hierarchy_path": file_name,
					}
					yield {"content": chunk_content, "metadata": metadata}
				except (ValueError, TypeError, KeyError, AttributeError) as e:
					logger.warning("Error creating whole-file chunk for %s: %s", file_path, e)

			# Then create more specific chunks for the individual entities
			yield from self._chunk_entity_recursive(
				root_entity, file_path, file_lines, git_hash, resolved_language, file_entity=root_entity
			)

		except (OSError, ValueError, TypeError, KeyError, AttributeError) as e:
			logger.debug("Failed to chunk file %s: %s", file_path, str(e))
			return
