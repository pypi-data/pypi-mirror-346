"""
This type stub file was generated by pyright.
"""

from typing import Any, Dict, Optional, Union
from pymilvus.orm.schema import CollectionSchema
from .constants import BulkFileType
from .local_bulk_writer import LocalBulkWriter

logger = ...
class RemoteBulkWriter(LocalBulkWriter):
    class S3ConnectParam:
        def __init__(self, bucket_name: str = ..., endpoint: Optional[str] = ..., access_key: Optional[str] = ..., secret_key: Optional[str] = ..., secure: bool = ..., session_token: Optional[str] = ..., region: Optional[str] = ..., http_client: Any = ..., credentials: Any = ...) -> None:
            ...
        
    
    
    ConnectParam = S3ConnectParam
    class AzureConnectParam:
        def __init__(self, container_name: str, conn_str: str, account_url: Optional[str] = ..., credential: Optional[Union[str, Dict[str, str]]] = ..., upload_chunk_size: int = ..., upload_concurrency: int = ...) -> None:
            """Connection parameters for Azure blob storage
            Args:
                container_name(str): The target container name

                conn_str(str): A connection string to an Azure Storage account,
                    which can be parsed to an account_url and a credential.
                    To generate a connection string, read this link:
                    https://learn.microsoft.com/en-us/azure/storage/common/storage-configure-connection-string

                account_url(str): A string in format like https://<storage-account>.blob.core.windows.net
                    Read this link for more info:
                    https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview

                credential: Account access key for the account, read this link for more info:
                    https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal#view-account-access-keys

                upload_chunk_size: If the blob size is larger than this value or unknown,
                    the blob is uploaded in chunks by parallel connections. This parameter is
                    passed to max_single_put_size of Azure. Read this link for more info:
                    https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-upload-python#specify-data-transfer-options-for-upload

                upload_concurrency: The maximum number of parallel connections to use when uploading
                    in chunks. This parameter is passed to max_concurrency of Azure.
                    Read this link for more info:
                    https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-upload-python#specify-data-transfer-options-for-upload
            """
            ...
        
    
    
    def __init__(self, schema: CollectionSchema, remote_path: str, connect_param: Optional[Union[S3ConnectParam, AzureConnectParam]], chunk_size: int = ..., file_type: BulkFileType = ..., config: Optional[dict] = ..., **kwargs) -> None:
        ...
    
    def __enter__(self): # -> Self:
        ...
    
    def __exit__(self, exc_type: object, exc_val: object, exc_tb: object): # -> None:
        ...
    
    def append_row(self, row: dict, **kwargs): # -> None:
        ...
    
    def commit(self, **kwargs): # -> None:
        ...
    
    @property
    def data_path(self): # -> Path:
        ...
    
    @property
    def batch_files(self): # -> list[Any]:
        ...
    


