"""
Wrangles data generated by `monitor_pid` from Herman's Code shell package.
"""

"""
NOTE for Developers

There are several challenges to parsing the data output by `ps`, the core of `monitor_pid`.
The data output has columns that are space delimmited. All headers are contiguous non-space characters.
However, some column headers are right-justified and others are left-justified.
The columns that are longer than their contents have at least one space to their left.
Since the format of the values doesn't follow the format of the headers, this makes it impossible to devise a programmatic way to delimit the data based on fixed widths.

Luckily, all columns also have contiguous values, except for two: `lstart` and `command` (which is the same as `args`). And `lstart` does have a predictable format. That means that if we order the columns specifiaclly, we can programmatically delimit the data.

VSCode can't display the full text beyond 230 characters, so for that we use the python function `narrow_text_table`.
"""

import argparse
import logging
import os
import pprint
import re
from pathlib import Path
from typing import List
# Third-party packages
import pandas as pd
# Local packages
from herman_code import __version__ as hc_version
from herman_code.code.narrow_text_table import narrow_text_table
from herman_code.code.utilities import (choose_path_to_log,
                                        get_timestamp,
                                        make_dir_path)


def tabulate():
    """
    """


if __name__ == "__main__":
    # >>> `Argparse` arguments >>>
    parser = argparse.ArgumentParser()

    # Arguments: Main
    parser.add_argument("--file_path",
                        type=Path,
                        help="""Path to the monitor log file.""")

    parser.add_argument("--ps_with_f_option",
                        action="store_true",
                        help="""If we are reading data from a call to `ps` that used the `-f` option, this will change how we parse the headers. The `-f` option adds 8 columns, which are probably already present elsewhere in the table.""")

    # Arguments: Meta-parameters
    parser.add_argument("--log_level",
                        default=10,
                        type=int,
                        help="""Increase output verbosity. See "logging" module's log level for valid values.""")

    arg_namespace = parser.parse_args()

    # Parsed arguments: Main
    file_path: Path = arg_namespace.file_path
    ps_with_f_option: bool = arg_namespace.ps_with_f_option

    # Parsed arguments: Meta-parameters
    log_level = arg_namespace.log_level

    # <<< `Argparse` arguments <<<

    # >>> Argument checks >>>
    # NOTE TODO Look into handling this natively with `argparse` by using `subcommands`. See "https://stackoverflow.com/questions/30457162/argparse-with-different-modes"
    pass
    # <<< Argument checks <<<

    # Variables: Path construction: General
    run_timestamp = get_timestamp()
    this_file_path = Path(__file__)
    this_file_stem = this_file_path.stem
    current_working_dir = Path(os.getcwd()).absolute()
    project_dir = current_working_dir
    data_dir = project_dir.joinpath("data")
    if data_dir:
        input_data_dir = data_dir.joinpath("input")
        intermediate_data_dir = data_dir.joinpath("intermediate")
        output_data_dir = data_dir.joinpath("output")
        if intermediate_data_dir:
            run_intermediate_dir = intermediate_data_dir.joinpath(this_file_stem, run_timestamp)
        if output_data_dir:
            run_output_dir = output_data_dir.joinpath(this_file_stem, run_timestamp)
    logs_dir = project_dir.joinpath("logs")
    if logs_dir:
        run_logs_dir = logs_dir.joinpath(this_file_stem)
    sql_dir = project_dir.joinpath("sql")

    # Variables: Path construction: Project-specific
    pass

    # Directory creation: General
    make_dir_path(run_intermediate_dir)
    make_dir_path(run_output_dir)
    make_dir_path(run_logs_dir)

    # Logging block
    logpath = run_logs_dir.joinpath(f"log {run_timestamp}.log")
    log_format = logging.Formatter("""[%(asctime)s][%(levelname)s](%(funcName)s): %(message)s""")

    logger = logging.getLogger(__name__)

    file_handler = logging.FileHandler(logpath)
    file_handler.setLevel(9)
    file_handler.setFormatter(log_format)

    stream_handler = logging.StreamHandler()
    stream_handler.setLevel(log_level)
    stream_handler.setFormatter(log_format)

    logger.addHandler(file_handler)
    logger.addHandler(stream_handler)

    logger.setLevel(9)

    logger.info(f"""Begin running "{choose_path_to_log(path=this_file_path, root_path=project_dir)}".""")
    logger.info(f"""Herman's Code version is "{hc_version}".""")
    logger.info(f"""All other paths will be reported in debugging relative to the current working directory: "{choose_path_to_log(path=project_dir, root_path=project_dir)}".""")

    arg_list = arg_namespace._get_args() + arg_namespace._get_kwargs()
    arg_list_string = pprint.pformat(arg_list)  # TODO Remove secrets from list to print, e.g., passwords.
    logger.info(f"""Script arguments:\n{arg_list_string}""")

    # >>> Begin script body >>>
    with open(file=file_path, mode="r") as file:
        text = file.read()

    # Extract monitor parameters
    pattern_1 = r""" >>> Monitoring Parameters >>>
num_keywords_used=(?P<mpar>[0-9]*)
 <<< Monitoring Parameters <<<"""
    rer_mpar = re.search(pattern=pattern_1,
                         string=text)
    mpar_num_keywords_used = int(rer_mpar.groupdict()["mpar"])

    # Extract snapshots
    pattern_2 = r""" >>> Snapshot Monitoring at (?P<timestamp>[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}-[0-9]{2}-[0-9]{2}) >>>
(?P<data>.+?)
 <<< Snapshot Monitoring at [0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}-[0-9]{2}-[0-9]{2} <<<"""
    rer_snapshots = re.findall(pattern=pattern_2,
                               string=text,
                               flags=re.DOTALL)

    # >>> Extract headers >>>
    text_table = rer_snapshots[0][1]
    lines = text_table.split("\n")
    headers_raw = text_table.split("\n")[0]
    values_raw = text_table.split("\n")[1]

    # Extract headers
    if ps_with_f_option:
        index_0 = 66
    else:
        index_0 = 0
    pattern_3 = r"([A-Z%_]+)"
    headers = re.findall(pattern=pattern_3,
                            string=headers_raw[index_0:])

    num_headers = len(headers)
    if len(headers) != mpar_num_keywords_used:
        message = f"""We expected {mpar_num_keywords_used} columns, but got {num_headers}."""
        logger.warning(message)
    # <<< Extract headers <<<

    # Parse data
    data_parsed = {}
    pattern_4 = r"^\s*"
    pattern_5 = r"\s+"
    pattern_6 = r"(;[A-Z][a-z]+\s+[A-Z][a-z]+\s+[0-9]{1,2}\s+[0-9]{2}:[0-9]{2}:[0-9]{2}\s+[0-9]{4})\s+"
    it_3 = 0
    for it_1, tu in enumerate(rer_snapshots, start=1):
        timestamp, text_table = tu
        lines: List[str] = text_table.split("\n")
        headers_raw_0, values_raw = lines[0], lines[1:]
        headers_raw_1: str = headers_raw_0[index_0:]
        values_raw: List[str] = values_raw
        for it_2, row_raw_0 in enumerate(values_raw, start=1):
            it_3 += 1
            row_raw_0: str = row_raw_0
            row_raw_1: str = row_raw_0[index_0:]
            row_raw_2: str = re.sub(pattern=pattern_4,
                                    repl="",
                                    string=row_raw_1)
            row_raw_3, nsubs = re.subn(pattern=pattern_5,
                                       repl=";",
                                       string=row_raw_2,
                                       count=num_headers - 2)
            row_raw_4 = re.sub(pattern=pattern_6,
                               repl=r"\1;",
                               string=row_raw_3)
            row_raw_5 = row_raw_4.split(";")
            row_parsed = {"timestamp": timestamp}
            row_parsed.update({header: value for header, value in zip(headers, row_raw_5)})
            data_parsed[it_3] = row_parsed

    # Save results
    data = pd.DataFrame.from_dict(data=data_parsed,
                                  orient="index")
    data.index.name = "index"
    outname = f"{file_path.stem}.CSV"
    outpath = run_output_dir.joinpath(outname)
    data.to_csv(path_or_buf=outpath)

    # Output location summary
    logger.info(f"""Script output is located in the following directory: "{choose_path_to_log(path=run_output_dir, root_path=project_dir)}".""")

    # <<< End script body <<<
    logger.info(f"""Finished running "{choose_path_to_log(path=this_file_path, root_path=project_dir)}".""")
