# Copyright (c) Huawei Technologies Co., Ltd. 2024-2025. All rights reserved.
# MindIE is licensed under Mulan PSL v2.
# You can use this software according to the terms and conditions of the Mulan PSL v2.
# You may obtain a copy of Mulan PSL v2 at:
#         http://license.coscl.org.cn/MulanPSL2
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
# See the Mulan PSL v2 for more details.

from typing import Tuple, Generator, List

import torch
from vllm.model_executor.layers.linear import UnquantizedLinearMethod
from vllm.config import VllmConfig
from vllm.distributed import (
    get_tensor_model_parallel_rank,
    get_tensor_model_parallel_world_size,
)
from vllm.model_executor.layers.layernorm import RMSNorm
from mindie_turbo.utils.logger import logger


def wrapper_weights_iterator(func):
    def _safetensors_weights_iterator(
        hf_weights_files: List[str],
    ) -> Generator[Tuple[str, torch.Tensor], None, None]:
        current_rank = get_tensor_model_parallel_rank()
        world_size = get_tensor_model_parallel_world_size()
        for name, weight in func(hf_weights_files):
            # The name of attention weights generated by msmodelslim
            # must be modified so that these weights can be loaded
            # into Attention module rather than LlamaAttention module.
            if "fa_" in name and ".attn." not in name:
                name = name.split(".")
                name.insert(name.index("self_attn") + 1, "attn")
                name = ".".join(name)
                # vLLM originally does not support splitting attention
                # weights with respect to TP ranks. We need split
                # weights manually here.
                if world_size <= 0:
                    raise ValueError("Expected world_size should be greater than 0"
                                    f"but got {world_size}.")
                split_size = weight.size(0) // world_size
                weight = weight[
                    current_rank * split_size:(current_rank + 1) * split_size
                ]

            yield name, weight

    return _safetensors_weights_iterator


def deepseek_gate_up_proj(layer_obj, idx):
    if idx == 0:
        return getattr(layer_obj, "gate_up_proj", None)
    shared = getattr(layer_obj, "shared_experts", None)
    return getattr(shared, "gate_up_proj", None) if shared is not None else None

MODEL_LAYER_MAPPING = {
    "LlamaModel": {
        "attn": {
            "layer_attr": "self_attn",
            "proj_attr": "qkv_proj",
            "norm_attr": "input_layernorm",
            "unquantized_type": UnquantizedLinearMethod,
        },
        "mlp": {
            "layer_attr": "mlp",
            "proj_attr": "gate_up_proj",
            "norm_attr": "post_attention_layernorm",
            "unquantized_type": UnquantizedLinearMethod,
        },
    },
    "Qwen2Model": {
        "attn": {
            "layer_attr": "self_attn",
            "proj_attr": "qkv_proj",
            "norm_attr": "input_layernorm",
            "unquantized_type": UnquantizedLinearMethod,
        },
        "mlp": {
            "layer_attr": "mlp",
            "proj_attr": "gate_up_proj",
            "norm_attr": "post_attention_layernorm",
            "unquantized_type": UnquantizedLinearMethod,
        },
    }
}


def wrapper_load_model(func):
    def postprocess_loading(self) -> None:
        func(self)
        
        def process_layer(layer, idx, mapping):
            def process_module(module_cfg, layer_obj):
                if module_cfg is None:
                    return
                
                module_obj = getattr(layer_obj, module_cfg["layer_attr"], None)
                if module_obj is None:
                    return
                
                proj_attr = module_cfg["proj_attr"]
                if callable(proj_attr):
                    proj = proj_attr(module_obj, idx)
                else:
                    proj = getattr(module_obj, proj_attr, None)
                
                norm = getattr(layer_obj, module_cfg["norm_attr"], None)
                
                if proj is None or norm is None:
                    return
                
                norm.ignore_anti = isinstance(proj.quant_method, module_cfg["unquantized_type"])
                if not norm.ignore_anti:
                    for param_name in ["input_scale", "input_offset"]:
                        if hasattr(proj, param_name):
                            param = getattr(proj, param_name)
                            norm.register_parameter(
                                param_name,
                                torch.nn.Parameter(param.clone(), requires_grad=False)
                            )

            process_module(mapping.get("attn"), layer)
            process_module(mapping.get("mlp"), layer)

        def is_enable(quant_description) -> bool:
            need_activate = False
            for name in quant_description.keys():
                if "norm.bias" in name:
                    need_activate = True
                    return need_activate
            return need_activate
        # 判断是否激活该patch
        try:
            if not is_enable(self.model.quant_config.quant_description):
                return
        except AttributeError:
            logger.info(f"Warning: load model patch do not enable, because it is not quantified and llm weights")
            return
        model_type = self.model.model.__class__.__name__
        mapping = MODEL_LAYER_MAPPING.get(model_type)
        
        if not mapping:
            logger.info(f"Warning: Model type '{model_type}' not found in MODEL_LAYER_MAPPING. Skipping layer mapping.")
            return

        for idx, layer in enumerate(self.model.model.layers):
            process_layer(layer, idx, mapping)

        if isinstance(self.model.model.norm, RMSNorm):
            self.model.model.norm.ignore_anti = True

    return postprocess_loading
