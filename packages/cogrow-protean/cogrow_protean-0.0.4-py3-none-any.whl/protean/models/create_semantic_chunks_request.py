# coding: utf-8

"""
    Protean API

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: 0.0.11
    Contact: support@cogrow.tech
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictFloat, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional, Union
from protean.models.breakpoint_strategy import BreakpointStrategy
from protean.models.combining_strategy import CombiningStrategy
from protean.models.similarity_strategy import SimilarityStrategy
from protean.models.text_splitting_strategy import TextSplittingStrategy
from typing import Optional, Set
from typing_extensions import Self

class CreateSemanticChunksRequest(BaseModel):
    """
    CreateSemanticChunksRequest
    """ # noqa: E501
    text: StrictStr = Field(description="The text to chunk.")
    embedding_model: Optional[StrictStr] = Field(default=None, description="The embedding model to use for chunking. If no value is provided then default embedding model of Protean platform is used.", alias="embeddingModel")
    text_splitting_strategy: Optional[TextSplittingStrategy] = Field(default=None, description="Semantic chunking splits text into sentences, which are later combined. This property determines how input text is split into sentences.", alias="textSplittingStrategy")
    combining_strategy: Optional[CombiningStrategy] = Field(default=None, description="Semantic chunking combines multiple sentences and then uses this combined unit for similarity comparison. This property determines how sentences are combined to form a unit for comparison.", alias="combiningStrategy")
    buffer_size: Optional[StrictInt] = Field(default=None, description="This property determines how many sentences are combined using CombiningStrategy.", alias="bufferSize")
    similarity_strategy: Optional[SimilarityStrategy] = Field(default=None, description="Semantic chunking calculates distance scores between subsequent combined units to measure how much the meaning shifts from one to the next, helping identify where the content becomes less cohesive. This property determines how to calculate similarity-score and distance to subsequent unit.", alias="similarityStrategy")
    breakpoint_strategy: Optional[BreakpointStrategy] = Field(default=None, description="Semantic chunking uses distance scores to determine the optimal boundaries between combined units. This property defines the method for analyzing those distance values to decide where meaningful breaks in the content should occur.", alias="breakpointStrategy")
    breakpoint_threshold_tuner: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="This property controls how sensitive the BreakpointStrategy is when identifying where to break the text into chunks. It determines what constitutes a 'significant enough' change or low enough similarity to qualify as a semantic boundary. Increase breakpointThresholdTuner to reduce splits (larger chunks). Decrease it to get more splits, (smaller chunks). Default Values: PERCENTILE - 85.0;  STANDARD_DEVIATION - 3.0; INTERQUARTILE - 0.05; GRADIENT - 95.0 ", alias="breakpointThresholdTuner")
    maximum_chunk_size: Optional[StrictInt] = Field(default=800, description="After semantic chunking, if the resulting chunk sizes are too big, the chunks will be broken further. This determines the number of tokes after which a chunk will be split.  It attempts to find a suitable break point ('.', '!' and '?')", alias="maximumChunkSize")
    __properties: ClassVar[List[str]] = ["text", "embeddingModel", "textSplittingStrategy", "combiningStrategy", "bufferSize", "similarityStrategy", "breakpointStrategy", "breakpointThresholdTuner", "maximumChunkSize"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of CreateSemanticChunksRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of CreateSemanticChunksRequest from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "text": obj.get("text"),
            "embeddingModel": obj.get("embeddingModel"),
            "textSplittingStrategy": obj.get("textSplittingStrategy"),
            "combiningStrategy": obj.get("combiningStrategy"),
            "bufferSize": obj.get("bufferSize"),
            "similarityStrategy": obj.get("similarityStrategy"),
            "breakpointStrategy": obj.get("breakpointStrategy"),
            "breakpointThresholdTuner": obj.get("breakpointThresholdTuner"),
            "maximumChunkSize": obj.get("maximumChunkSize") if obj.get("maximumChunkSize") is not None else 800
        })
        return _obj


