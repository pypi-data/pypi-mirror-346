# This file was auto-generated by Fern from our API Definition.

import typing
from ..core.client_wrapper import SyncClientWrapper
from ..types.provided_processor_output import ProvidedProcessorOutput
from ..core.request_options import RequestOptions
from .types.evaluation_set_item_create_response import EvaluationSetItemCreateResponse
from ..core.serialization import convert_and_respect_annotation_metadata
from ..core.pydantic_utilities import parse_obj_as
from ..errors.bad_request_error import BadRequestError
from ..errors.unauthorized_error import UnauthorizedError
from ..types.error import Error
from json.decoder import JSONDecodeError
from ..core.api_error import ApiError
from .types.evaluation_set_item_update_response import EvaluationSetItemUpdateResponse
from ..core.jsonable_encoder import jsonable_encoder
from ..errors.not_found_error import NotFoundError
from .types.evaluation_set_item_create_batch_request_items_item import EvaluationSetItemCreateBatchRequestItemsItem
from .types.evaluation_set_item_create_batch_response import EvaluationSetItemCreateBatchResponse
from ..core.client_wrapper import AsyncClientWrapper

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class EvaluationSetItemClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def create(
        self,
        *,
        evaluation_set_id: str,
        file_id: str,
        expected_output: ProvidedProcessorOutput,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetItemCreateResponse:
        """
        Evaluation set items are the individual files and expected outputs that are used to evaluate the performance of a given processor in Extend. This endpoint will create a new evaluation set item in Extend, which will be used during an evaluation run.

        Best Practices for Outputs in Evaluation Sets:
        - **Configure First, Output Later**
          - Always create and finalize your processor configuration before creating evaluation sets
          - Field IDs in outputs must match those defined in your processor configuration
        - **Type Consistency**
          - Ensure output types exactly match your processor configuration
          - For example, if a field is configured as "currency", don't submit a simple number value
        - **Field IDs**
          - Use the exact field IDs from your processor configuration
          - Create your own semantic IDs instead in the configs for each field/type instead of using the generated ones
        - **Value**
          - Remember that all results are inside the value key of a result object, except the values within nested structures.

        Parameters
        ----------
        evaluation_set_id : str
            The ID of the evaluation set to add the item to. The ID will start with "ev_".

            Example: `"ev_Xj8mK2pL9nR4vT7qY5wZ"`

        file_id : str
            Extend's internal ID for the file. It will always start with "file_".

            Example: `"file_xK9mLPqRtN3vS8wF5hB2cQ"`

        expected_output : ProvidedProcessorOutput
            The expected output that will be used to evaluate the processor's performance.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItemCreateResponse
            Successfully created evaluation set item

        Examples
        --------
        from extendconfig import Extend, ProvidedJsonOutput

        client = Extend(
            extend_api_version="YOUR_EXTEND_API_VERSION",
            token="YOUR_TOKEN",
        )
        client.evaluation_set_item.create(
            evaluation_set_id="evaluation_set_id_here",
            file_id="file_id_here",
            expected_output=ProvidedJsonOutput(
                value={"key": "value"},
            ),
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "evaluation_set_items",
            method="POST",
            json={
                "evaluationSetId": evaluation_set_id,
                "fileId": file_id,
                "expectedOutput": convert_and_respect_annotation_metadata(
                    object_=expected_output, annotation=ProvidedProcessorOutput, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationSetItemCreateResponse,
                    parse_obj_as(
                        type_=EvaluationSetItemCreateResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(
                        typing.Optional[typing.Any],
                        parse_obj_as(
                            type_=typing.Optional[typing.Any],  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update(
        self,
        id: str,
        *,
        expected_output: ProvidedProcessorOutput,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetItemUpdateResponse:
        """
        If you need to change the expected output for a given evaluation set item, you can use this endpoint to update the item. This can be useful if you need to correct an error in the expected output or if the output of the processor has changed.

        Parameters
        ----------
        id : str
            The ID of the evaluation set item to update. The ID will start with "evi_".

            Example: `"evi_kR9mNP12Qw4yTv8BdR3H"`

        expected_output : ProvidedProcessorOutput
            The expected output of the processor when run against the file

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItemUpdateResponse
            Successfully updated evaluation set item

        Examples
        --------
        from extendconfig import Extend, ProvidedJsonOutput

        client = Extend(
            extend_api_version="YOUR_EXTEND_API_VERSION",
            token="YOUR_TOKEN",
        )
        client.evaluation_set_item.update(
            id="evaluation_set_item_id_here",
            expected_output=ProvidedJsonOutput(
                value={"key": "value"},
            ),
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"evaluation_set_items/{jsonable_encoder(id)}",
            method="POST",
            json={
                "expectedOutput": convert_and_respect_annotation_metadata(
                    object_=expected_output, annotation=ProvidedProcessorOutput, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationSetItemUpdateResponse,
                    parse_obj_as(
                        type_=EvaluationSetItemUpdateResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(
                        typing.Optional[typing.Any],
                        parse_obj_as(
                            type_=typing.Optional[typing.Any],  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 404:
                raise NotFoundError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_batch(
        self,
        *,
        evaluation_set_id: str,
        items: typing.Sequence[EvaluationSetItemCreateBatchRequestItemsItem],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetItemCreateBatchResponse:
        """
        If you have a large number of files that you need to add to an evaluation set, you can use this endpoint to create multiple evaluation set items at once. This can be useful if you have a large dataset that you need to evaluate the performance of a processor against.

        Note: you still need to create each File first using the file API.

        Parameters
        ----------
        evaluation_set_id : str
            The ID of the evaluation set to add the items to. The ID will start with "ev_".

            Example: `"ev_2LcgeY_mp2T5yPaEuq5Lw"`

        items : typing.Sequence[EvaluationSetItemCreateBatchRequestItemsItem]
            An array of objects representing the evaluation set items to create

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItemCreateBatchResponse
            Successfully created evaluation set items

        Examples
        --------
        from extendconfig import Extend, ProvidedJsonOutput
        from extendconfig.evaluation_set_item import (
            EvaluationSetItemCreateBatchRequestItemsItem,
        )

        client = Extend(
            extend_api_version="YOUR_EXTEND_API_VERSION",
            token="YOUR_TOKEN",
        )
        client.evaluation_set_item.create_batch(
            evaluation_set_id="evaluation_set_id_here",
            items=[
                EvaluationSetItemCreateBatchRequestItemsItem(
                    file_id="file_id_here",
                    expected_output=ProvidedJsonOutput(
                        value={"key": "value"},
                    ),
                )
            ],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "evaluation_set_items/bulk",
            method="POST",
            json={
                "evaluationSetId": evaluation_set_id,
                "items": convert_and_respect_annotation_metadata(
                    object_=items,
                    annotation=typing.Sequence[EvaluationSetItemCreateBatchRequestItemsItem],
                    direction="write",
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationSetItemCreateBatchResponse,
                    parse_obj_as(
                        type_=EvaluationSetItemCreateBatchResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(
                        typing.Optional[typing.Any],
                        parse_obj_as(
                            type_=typing.Optional[typing.Any],  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncEvaluationSetItemClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def create(
        self,
        *,
        evaluation_set_id: str,
        file_id: str,
        expected_output: ProvidedProcessorOutput,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetItemCreateResponse:
        """
        Evaluation set items are the individual files and expected outputs that are used to evaluate the performance of a given processor in Extend. This endpoint will create a new evaluation set item in Extend, which will be used during an evaluation run.

        Best Practices for Outputs in Evaluation Sets:
        - **Configure First, Output Later**
          - Always create and finalize your processor configuration before creating evaluation sets
          - Field IDs in outputs must match those defined in your processor configuration
        - **Type Consistency**
          - Ensure output types exactly match your processor configuration
          - For example, if a field is configured as "currency", don't submit a simple number value
        - **Field IDs**
          - Use the exact field IDs from your processor configuration
          - Create your own semantic IDs instead in the configs for each field/type instead of using the generated ones
        - **Value**
          - Remember that all results are inside the value key of a result object, except the values within nested structures.

        Parameters
        ----------
        evaluation_set_id : str
            The ID of the evaluation set to add the item to. The ID will start with "ev_".

            Example: `"ev_Xj8mK2pL9nR4vT7qY5wZ"`

        file_id : str
            Extend's internal ID for the file. It will always start with "file_".

            Example: `"file_xK9mLPqRtN3vS8wF5hB2cQ"`

        expected_output : ProvidedProcessorOutput
            The expected output that will be used to evaluate the processor's performance.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItemCreateResponse
            Successfully created evaluation set item

        Examples
        --------
        import asyncio

        from extendconfig import AsyncExtend, ProvidedJsonOutput

        client = AsyncExtend(
            extend_api_version="YOUR_EXTEND_API_VERSION",
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.evaluation_set_item.create(
                evaluation_set_id="evaluation_set_id_here",
                file_id="file_id_here",
                expected_output=ProvidedJsonOutput(
                    value={"key": "value"},
                ),
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "evaluation_set_items",
            method="POST",
            json={
                "evaluationSetId": evaluation_set_id,
                "fileId": file_id,
                "expectedOutput": convert_and_respect_annotation_metadata(
                    object_=expected_output, annotation=ProvidedProcessorOutput, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationSetItemCreateResponse,
                    parse_obj_as(
                        type_=EvaluationSetItemCreateResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(
                        typing.Optional[typing.Any],
                        parse_obj_as(
                            type_=typing.Optional[typing.Any],  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update(
        self,
        id: str,
        *,
        expected_output: ProvidedProcessorOutput,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetItemUpdateResponse:
        """
        If you need to change the expected output for a given evaluation set item, you can use this endpoint to update the item. This can be useful if you need to correct an error in the expected output or if the output of the processor has changed.

        Parameters
        ----------
        id : str
            The ID of the evaluation set item to update. The ID will start with "evi_".

            Example: `"evi_kR9mNP12Qw4yTv8BdR3H"`

        expected_output : ProvidedProcessorOutput
            The expected output of the processor when run against the file

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItemUpdateResponse
            Successfully updated evaluation set item

        Examples
        --------
        import asyncio

        from extendconfig import AsyncExtend, ProvidedJsonOutput

        client = AsyncExtend(
            extend_api_version="YOUR_EXTEND_API_VERSION",
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.evaluation_set_item.update(
                id="evaluation_set_item_id_here",
                expected_output=ProvidedJsonOutput(
                    value={"key": "value"},
                ),
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"evaluation_set_items/{jsonable_encoder(id)}",
            method="POST",
            json={
                "expectedOutput": convert_and_respect_annotation_metadata(
                    object_=expected_output, annotation=ProvidedProcessorOutput, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationSetItemUpdateResponse,
                    parse_obj_as(
                        type_=EvaluationSetItemUpdateResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(
                        typing.Optional[typing.Any],
                        parse_obj_as(
                            type_=typing.Optional[typing.Any],  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 404:
                raise NotFoundError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_batch(
        self,
        *,
        evaluation_set_id: str,
        items: typing.Sequence[EvaluationSetItemCreateBatchRequestItemsItem],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetItemCreateBatchResponse:
        """
        If you have a large number of files that you need to add to an evaluation set, you can use this endpoint to create multiple evaluation set items at once. This can be useful if you have a large dataset that you need to evaluate the performance of a processor against.

        Note: you still need to create each File first using the file API.

        Parameters
        ----------
        evaluation_set_id : str
            The ID of the evaluation set to add the items to. The ID will start with "ev_".

            Example: `"ev_2LcgeY_mp2T5yPaEuq5Lw"`

        items : typing.Sequence[EvaluationSetItemCreateBatchRequestItemsItem]
            An array of objects representing the evaluation set items to create

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItemCreateBatchResponse
            Successfully created evaluation set items

        Examples
        --------
        import asyncio

        from extendconfig import AsyncExtend, ProvidedJsonOutput
        from extendconfig.evaluation_set_item import (
            EvaluationSetItemCreateBatchRequestItemsItem,
        )

        client = AsyncExtend(
            extend_api_version="YOUR_EXTEND_API_VERSION",
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.evaluation_set_item.create_batch(
                evaluation_set_id="evaluation_set_id_here",
                items=[
                    EvaluationSetItemCreateBatchRequestItemsItem(
                        file_id="file_id_here",
                        expected_output=ProvidedJsonOutput(
                            value={"key": "value"},
                        ),
                    )
                ],
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "evaluation_set_items/bulk",
            method="POST",
            json={
                "evaluationSetId": evaluation_set_id,
                "items": convert_and_respect_annotation_metadata(
                    object_=items,
                    annotation=typing.Sequence[EvaluationSetItemCreateBatchRequestItemsItem],
                    direction="write",
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationSetItemCreateBatchResponse,
                    parse_obj_as(
                        type_=EvaluationSetItemCreateBatchResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(
                        typing.Optional[typing.Any],
                        parse_obj_as(
                            type_=typing.Optional[typing.Any],  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
