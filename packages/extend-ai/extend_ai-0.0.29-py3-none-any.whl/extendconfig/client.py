# This file was auto-generated by Fern from our API Definition.

import typing
from .environment import ExtendEnvironment
from .types.api_version_enum import ApiVersionEnum
import httpx
from .core.client_wrapper import SyncClientWrapper
from .workflow_run.client import WorkflowRunClient
from .batch_workflow_run.client import BatchWorkflowRunClient
from .processor_run.client import ProcessorRunClient
from .processor.client import ProcessorClient
from .processor_version.client import ProcessorVersionClient
from .file.client import FileClient
from .file_endpoints.client import FileEndpointsClient
from .evaluation_set.client import EvaluationSetClient
from .evaluation_set_item.client import EvaluationSetItemClient
from .workflow_run_output.client import WorkflowRunOutputClient
from .batch_processor_run.client import BatchProcessorRunClient
from .workflow.client import WorkflowClient
from .types.file4 import File4
from .types.json_object import JsonObject
from .core.request_options import RequestOptions
from .types.run_workflow_response import RunWorkflowResponse
from .core.serialization import convert_and_respect_annotation_metadata
from .core.pydantic_utilities import parse_obj_as
from .errors.bad_request_error import BadRequestError
from .errors.unauthorized_error import UnauthorizedError
from .types.error import Error
from json.decoder import JSONDecodeError
from .core.api_error import ApiError
from .types.processor_id import ProcessorId
from .types.file3processor import File3Processor
from .types.run_processor_request_config import RunProcessorRequestConfig
from .types.run_processor_response import RunProcessorResponse
from .errors.not_found_error import NotFoundError
from .types.parse_request_file import ParseRequestFile
from .types.parse_config import ParseConfig
from .types.parse_response import ParseResponse
from .errors.unprocessable_entity_error import UnprocessableEntityError
from .core.client_wrapper import AsyncClientWrapper
from .workflow_run.client import AsyncWorkflowRunClient
from .batch_workflow_run.client import AsyncBatchWorkflowRunClient
from .processor_run.client import AsyncProcessorRunClient
from .processor.client import AsyncProcessorClient
from .processor_version.client import AsyncProcessorVersionClient
from .file.client import AsyncFileClient
from .file_endpoints.client import AsyncFileEndpointsClient
from .evaluation_set.client import AsyncEvaluationSetClient
from .evaluation_set_item.client import AsyncEvaluationSetItemClient
from .workflow_run_output.client import AsyncWorkflowRunOutputClient
from .batch_processor_run.client import AsyncBatchProcessorRunClient
from .workflow.client import AsyncWorkflowClient

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class Extend:
    """
    Use this class to access the different functions within the SDK. You can instantiate any number of clients with different configuration that will propagate to these functions.

    Parameters
    ----------
    base_url : typing.Optional[str]
        The base url to use for requests from the client.

    environment : ExtendEnvironment
        The environment to use for requests from the client. from .environment import ExtendEnvironment



        Defaults to ExtendEnvironment.PRODUCTION



    extend_api_version : typing.Optional[ApiVersionEnum]
    token : typing.Union[str, typing.Callable[[], str]]
    timeout : typing.Optional[float]
        The timeout to be used, in seconds, for requests. By default the timeout is 60 seconds, unless a custom httpx client is used, in which case this default is not enforced.

    follow_redirects : typing.Optional[bool]
        Whether the default httpx client follows redirects or not, this is irrelevant if a custom httpx client is passed in.

    httpx_client : typing.Optional[httpx.Client]
        The httpx client to use for making requests, a preconfigured client is used by default, however this is useful should you want to pass in any custom httpx configuration.

    Examples
    --------
    from extendconfig import Extend

    client = Extend(
        extend_api_version="YOUR_EXTEND_API_VERSION",
        token="YOUR_TOKEN",
    )
    """

    def __init__(
        self,
        *,
        base_url: typing.Optional[str] = None,
        environment: ExtendEnvironment = ExtendEnvironment.PRODUCTION,
        extend_api_version: typing.Optional[ApiVersionEnum] = None,
        token: typing.Union[str, typing.Callable[[], str]],
        timeout: typing.Optional[float] = None,
        follow_redirects: typing.Optional[bool] = True,
        httpx_client: typing.Optional[httpx.Client] = None,
    ):
        _defaulted_timeout = timeout if timeout is not None else 60 if httpx_client is None else None
        self._client_wrapper = SyncClientWrapper(
            base_url=_get_base_url(base_url=base_url, environment=environment),
            extend_api_version=extend_api_version,
            token=token,
            httpx_client=httpx_client
            if httpx_client is not None
            else httpx.Client(timeout=_defaulted_timeout, follow_redirects=follow_redirects)
            if follow_redirects is not None
            else httpx.Client(timeout=_defaulted_timeout),
            timeout=_defaulted_timeout,
        )
        self.workflow_run = WorkflowRunClient(client_wrapper=self._client_wrapper)
        self.batch_workflow_run = BatchWorkflowRunClient(client_wrapper=self._client_wrapper)
        self.processor_run = ProcessorRunClient(client_wrapper=self._client_wrapper)
        self.processor = ProcessorClient(client_wrapper=self._client_wrapper)
        self.processor_version = ProcessorVersionClient(client_wrapper=self._client_wrapper)
        self.file = FileClient(client_wrapper=self._client_wrapper)
        self.file_endpoints = FileEndpointsClient(client_wrapper=self._client_wrapper)
        self.evaluation_set = EvaluationSetClient(client_wrapper=self._client_wrapper)
        self.evaluation_set_item = EvaluationSetItemClient(client_wrapper=self._client_wrapper)
        self.workflow_run_output = WorkflowRunOutputClient(client_wrapper=self._client_wrapper)
        self.batch_processor_run = BatchProcessorRunClient(client_wrapper=self._client_wrapper)
        self.workflow = WorkflowClient(client_wrapper=self._client_wrapper)

    def run_workflow(
        self,
        *,
        workflow_id: str,
        files: typing.Optional[typing.Sequence[File4]] = OMIT,
        raw_texts: typing.Optional[typing.Sequence[str]] = OMIT,
        version: typing.Optional[str] = OMIT,
        priority: typing.Optional[int] = OMIT,
        metadata: typing.Optional[JsonObject] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> RunWorkflowResponse:
        """
        Run a Workflow with files. A Workflow is a sequence of steps that process files and data in a specific order to achieve a desired outcome. A WorkflowRun will be created for each file processed. A WorkflowRun represents a single execution of a workflow against a file.

        Parameters
        ----------
        workflow_id : str
            The ID of the workflow to run. The ID will start with "workflow". This ID can be found viewing the workflow on the Extend platform.

            Example: `"workflow_BMdfq_yWM3sT-ZzvCnA3f"`

        files : typing.Optional[typing.Sequence[File4]]
            An array of files to process through the workflow. Either the `files` array or `rawTexts` array must be provided. Supported file types can be found [here](/developers/guides/supported-file-types).

        raw_texts : typing.Optional[typing.Sequence[str]]
            An array of raw strings. Can be used in place of files when passing raw data. The raw data will be converted to `.txt` files and run through the workflow. If the data follows a specific format, it is recommended to use the files parameter instead. Either `files` or `rawTexts` must be provided.

        version : typing.Optional[str]
            An optional version of the workflow that files will be run through. This number can be found when viewing the workflow on the Extend platform. When a version number is not supplied, the most recent published version of the workflow will be used. If no published versions exist, the draft version will be used. To run the `"draft"` version of a workflow, use `"draft"` as the version.

            Examples:
            - `"3"` - Run version 3 of the workflow
            - `"draft"` - Run the draft version of the workflow

        priority : typing.Optional[int]
            An optional value used to determine the relative order of WorkflowRuns when rate limiting is in effect. Lower values will be prioritized before higher values.

        metadata : typing.Optional[JsonObject]
            A optional metadata object that can be assigned to a specific WorkflowRun to help identify it. It will be returned in the response and webhooks. You can place any arbitrary `key : value` pairs in this object.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        RunWorkflowResponse
            Successfully created workflow runs

        Examples
        --------
        from extendconfig import Extend

        client = Extend(
            extend_api_version="YOUR_EXTEND_API_VERSION",
            token="YOUR_TOKEN",
        )
        client.run_workflow(
            workflow_id="workflow_id_here",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "workflow_runs",
            method="POST",
            json={
                "workflowId": workflow_id,
                "files": convert_and_respect_annotation_metadata(
                    object_=files, annotation=typing.Sequence[File4], direction="write"
                ),
                "rawTexts": raw_texts,
                "version": version,
                "priority": priority,
                "metadata": metadata,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    RunWorkflowResponse,
                    parse_obj_as(
                        type_=RunWorkflowResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(
                        typing.Optional[typing.Any],
                        parse_obj_as(
                            type_=typing.Optional[typing.Any],  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def run_processor(
        self,
        *,
        processor_id: ProcessorId,
        version: typing.Optional[str] = OMIT,
        file: typing.Optional[File3Processor] = OMIT,
        raw_text: typing.Optional[str] = OMIT,
        priority: typing.Optional[int] = OMIT,
        metadata: typing.Optional[JsonObject] = OMIT,
        config: typing.Optional[RunProcessorRequestConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> RunProcessorResponse:
        """
        Run processors (extraction, classification, splitting, etc.) on a given document.

        In general, the recommended way to integrate with Extend in production is via workflows, using the [Run Workflow](/developers/api-reference/workflow-endpoints/run-workflow) endpoint. This is due to several factors:
        * file parsing/pre-processing will automatically be reused across multiple processors, which will give you simplicity and cost savings given that many use cases will require multiple processors to be run on the same document.
        * workflows provide dedicated human in the loop document review, when needed.
        * workflows allow you to model and manage your pipeline with a single endpoint and corresponding UI for modeling and monitoring.

        However, there are a number of legitimate use cases and systems where it might be easier to model the pipeline via code and run processors directly. This endpoint is provided for this purpose.

        Similar to workflow runs, processor runs are asynchronous and will return a status of `PROCESSING` until the run is complete. You can [configure webhooks](/developers/webhooks/configuration) to receive notifications when a processor run is complete or failed.

        Parameters
        ----------
        processor_id : ProcessorId

        version : typing.Optional[str]
            An optional version of the processor to use. When not supplied, the most recent published version of the processor will be used. Special values include:
            - `"latest"` for the most recent published version. If there are no published versions, the draft version will be used.
            - `"draft"` for the draft version.
            - Specific version numbers corresponding to versions your team has published, e.g. `"1.0"`, `"2.2"`, etc.

        file : typing.Optional[File3Processor]
            The file to be processed. One of `file` or `rawText` must be provided. Supported file types can be found [here](/developers/guides/supported-file-types).

        raw_text : typing.Optional[str]
            A raw string to be processed. Can be used in place of file when passing raw text data streams. One of `file` or `rawText` must be provided.

        priority : typing.Optional[int]
            An optional value used to determine the relative order of ProcessorRuns when rate limiting is in effect. Lower values will be prioritized before higher values.

        metadata : typing.Optional[JsonObject]
            An optional object that can be passed in to identify the run of the document processor. It will be returned back to you in the response and webhooks.

        config : typing.Optional[RunProcessorRequestConfig]
            The configuration for the processor run. If this is provided, this config will be used. If not provided, the config for the specific version you provide will be used. The type of configuration must match the processor type.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        RunProcessorResponse
            Successfully created processor run

        Examples
        --------
        from extendconfig import Extend

        client = Extend(
            extend_api_version="YOUR_EXTEND_API_VERSION",
            token="YOUR_TOKEN",
        )
        client.run_processor(
            processor_id="processor_id_here",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "processor_runs",
            method="POST",
            json={
                "processorId": processor_id,
                "version": version,
                "file": convert_and_respect_annotation_metadata(
                    object_=file, annotation=File3Processor, direction="write"
                ),
                "rawText": raw_text,
                "priority": priority,
                "metadata": metadata,
                "config": convert_and_respect_annotation_metadata(
                    object_=config, annotation=RunProcessorRequestConfig, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    RunProcessorResponse,
                    parse_obj_as(
                        type_=RunProcessorResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(
                        typing.Optional[typing.Any],
                        parse_obj_as(
                            type_=typing.Optional[typing.Any],  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 404:
                raise NotFoundError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def parse(
        self, *, file: ParseRequestFile, config: ParseConfig, request_options: typing.Optional[RequestOptions] = None
    ) -> ParseResponse:
        """
        Parse files to get cleaned, chunked target content (e.g. markdown).

        The Parse endpoint allows you to convert documents into structured, machine-readable formats with fine-grained control over the parsing process. This endpoint is ideal for extracting cleaned document content to be used as context for downstream processing, e.g. RAG pipelines, custom ingestion pipelines, embeddings classification, etc.

        Unlike processor and workflow runs, parsing is a synchronous endpoint and returns the parsed content in the response. Expected latency depends primarily on file size. This makes it suitable for workflows where you need immediate access to document content without waiting for asynchronous processing.

        For more details, see the [Parse File guide](/developers/guides/parse).

        Parameters
        ----------
        file : ParseRequestFile
            A file object containing either a URL or a fileId.

        config : ParseConfig

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ParseResponse
            Successfully parsed file

        Examples
        --------
        from extendconfig import Extend, ParseConfig, ParseRequestFile

        client = Extend(
            extend_api_version="YOUR_EXTEND_API_VERSION",
            token="YOUR_TOKEN",
        )
        client.parse(
            file=ParseRequestFile(),
            config=ParseConfig(),
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "parse",
            method="POST",
            json={
                "file": convert_and_respect_annotation_metadata(
                    object_=file, annotation=ParseRequestFile, direction="write"
                ),
                "config": convert_and_respect_annotation_metadata(
                    object_=config, annotation=ParseConfig, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ParseResponse,
                    parse_obj_as(
                        type_=ParseResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(
                        typing.Optional[typing.Any],
                        parse_obj_as(
                            type_=typing.Optional[typing.Any],  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncExtend:
    """
    Use this class to access the different functions within the SDK. You can instantiate any number of clients with different configuration that will propagate to these functions.

    Parameters
    ----------
    base_url : typing.Optional[str]
        The base url to use for requests from the client.

    environment : ExtendEnvironment
        The environment to use for requests from the client. from .environment import ExtendEnvironment



        Defaults to ExtendEnvironment.PRODUCTION



    extend_api_version : typing.Optional[ApiVersionEnum]
    token : typing.Union[str, typing.Callable[[], str]]
    timeout : typing.Optional[float]
        The timeout to be used, in seconds, for requests. By default the timeout is 60 seconds, unless a custom httpx client is used, in which case this default is not enforced.

    follow_redirects : typing.Optional[bool]
        Whether the default httpx client follows redirects or not, this is irrelevant if a custom httpx client is passed in.

    httpx_client : typing.Optional[httpx.AsyncClient]
        The httpx client to use for making requests, a preconfigured client is used by default, however this is useful should you want to pass in any custom httpx configuration.

    Examples
    --------
    from extendconfig import AsyncExtend

    client = AsyncExtend(
        extend_api_version="YOUR_EXTEND_API_VERSION",
        token="YOUR_TOKEN",
    )
    """

    def __init__(
        self,
        *,
        base_url: typing.Optional[str] = None,
        environment: ExtendEnvironment = ExtendEnvironment.PRODUCTION,
        extend_api_version: typing.Optional[ApiVersionEnum] = None,
        token: typing.Union[str, typing.Callable[[], str]],
        timeout: typing.Optional[float] = None,
        follow_redirects: typing.Optional[bool] = True,
        httpx_client: typing.Optional[httpx.AsyncClient] = None,
    ):
        _defaulted_timeout = timeout if timeout is not None else 60 if httpx_client is None else None
        self._client_wrapper = AsyncClientWrapper(
            base_url=_get_base_url(base_url=base_url, environment=environment),
            extend_api_version=extend_api_version,
            token=token,
            httpx_client=httpx_client
            if httpx_client is not None
            else httpx.AsyncClient(timeout=_defaulted_timeout, follow_redirects=follow_redirects)
            if follow_redirects is not None
            else httpx.AsyncClient(timeout=_defaulted_timeout),
            timeout=_defaulted_timeout,
        )
        self.workflow_run = AsyncWorkflowRunClient(client_wrapper=self._client_wrapper)
        self.batch_workflow_run = AsyncBatchWorkflowRunClient(client_wrapper=self._client_wrapper)
        self.processor_run = AsyncProcessorRunClient(client_wrapper=self._client_wrapper)
        self.processor = AsyncProcessorClient(client_wrapper=self._client_wrapper)
        self.processor_version = AsyncProcessorVersionClient(client_wrapper=self._client_wrapper)
        self.file = AsyncFileClient(client_wrapper=self._client_wrapper)
        self.file_endpoints = AsyncFileEndpointsClient(client_wrapper=self._client_wrapper)
        self.evaluation_set = AsyncEvaluationSetClient(client_wrapper=self._client_wrapper)
        self.evaluation_set_item = AsyncEvaluationSetItemClient(client_wrapper=self._client_wrapper)
        self.workflow_run_output = AsyncWorkflowRunOutputClient(client_wrapper=self._client_wrapper)
        self.batch_processor_run = AsyncBatchProcessorRunClient(client_wrapper=self._client_wrapper)
        self.workflow = AsyncWorkflowClient(client_wrapper=self._client_wrapper)

    async def run_workflow(
        self,
        *,
        workflow_id: str,
        files: typing.Optional[typing.Sequence[File4]] = OMIT,
        raw_texts: typing.Optional[typing.Sequence[str]] = OMIT,
        version: typing.Optional[str] = OMIT,
        priority: typing.Optional[int] = OMIT,
        metadata: typing.Optional[JsonObject] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> RunWorkflowResponse:
        """
        Run a Workflow with files. A Workflow is a sequence of steps that process files and data in a specific order to achieve a desired outcome. A WorkflowRun will be created for each file processed. A WorkflowRun represents a single execution of a workflow against a file.

        Parameters
        ----------
        workflow_id : str
            The ID of the workflow to run. The ID will start with "workflow". This ID can be found viewing the workflow on the Extend platform.

            Example: `"workflow_BMdfq_yWM3sT-ZzvCnA3f"`

        files : typing.Optional[typing.Sequence[File4]]
            An array of files to process through the workflow. Either the `files` array or `rawTexts` array must be provided. Supported file types can be found [here](/developers/guides/supported-file-types).

        raw_texts : typing.Optional[typing.Sequence[str]]
            An array of raw strings. Can be used in place of files when passing raw data. The raw data will be converted to `.txt` files and run through the workflow. If the data follows a specific format, it is recommended to use the files parameter instead. Either `files` or `rawTexts` must be provided.

        version : typing.Optional[str]
            An optional version of the workflow that files will be run through. This number can be found when viewing the workflow on the Extend platform. When a version number is not supplied, the most recent published version of the workflow will be used. If no published versions exist, the draft version will be used. To run the `"draft"` version of a workflow, use `"draft"` as the version.

            Examples:
            - `"3"` - Run version 3 of the workflow
            - `"draft"` - Run the draft version of the workflow

        priority : typing.Optional[int]
            An optional value used to determine the relative order of WorkflowRuns when rate limiting is in effect. Lower values will be prioritized before higher values.

        metadata : typing.Optional[JsonObject]
            A optional metadata object that can be assigned to a specific WorkflowRun to help identify it. It will be returned in the response and webhooks. You can place any arbitrary `key : value` pairs in this object.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        RunWorkflowResponse
            Successfully created workflow runs

        Examples
        --------
        import asyncio

        from extendconfig import AsyncExtend

        client = AsyncExtend(
            extend_api_version="YOUR_EXTEND_API_VERSION",
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.run_workflow(
                workflow_id="workflow_id_here",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "workflow_runs",
            method="POST",
            json={
                "workflowId": workflow_id,
                "files": convert_and_respect_annotation_metadata(
                    object_=files, annotation=typing.Sequence[File4], direction="write"
                ),
                "rawTexts": raw_texts,
                "version": version,
                "priority": priority,
                "metadata": metadata,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    RunWorkflowResponse,
                    parse_obj_as(
                        type_=RunWorkflowResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(
                        typing.Optional[typing.Any],
                        parse_obj_as(
                            type_=typing.Optional[typing.Any],  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def run_processor(
        self,
        *,
        processor_id: ProcessorId,
        version: typing.Optional[str] = OMIT,
        file: typing.Optional[File3Processor] = OMIT,
        raw_text: typing.Optional[str] = OMIT,
        priority: typing.Optional[int] = OMIT,
        metadata: typing.Optional[JsonObject] = OMIT,
        config: typing.Optional[RunProcessorRequestConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> RunProcessorResponse:
        """
        Run processors (extraction, classification, splitting, etc.) on a given document.

        In general, the recommended way to integrate with Extend in production is via workflows, using the [Run Workflow](/developers/api-reference/workflow-endpoints/run-workflow) endpoint. This is due to several factors:
        * file parsing/pre-processing will automatically be reused across multiple processors, which will give you simplicity and cost savings given that many use cases will require multiple processors to be run on the same document.
        * workflows provide dedicated human in the loop document review, when needed.
        * workflows allow you to model and manage your pipeline with a single endpoint and corresponding UI for modeling and monitoring.

        However, there are a number of legitimate use cases and systems where it might be easier to model the pipeline via code and run processors directly. This endpoint is provided for this purpose.

        Similar to workflow runs, processor runs are asynchronous and will return a status of `PROCESSING` until the run is complete. You can [configure webhooks](/developers/webhooks/configuration) to receive notifications when a processor run is complete or failed.

        Parameters
        ----------
        processor_id : ProcessorId

        version : typing.Optional[str]
            An optional version of the processor to use. When not supplied, the most recent published version of the processor will be used. Special values include:
            - `"latest"` for the most recent published version. If there are no published versions, the draft version will be used.
            - `"draft"` for the draft version.
            - Specific version numbers corresponding to versions your team has published, e.g. `"1.0"`, `"2.2"`, etc.

        file : typing.Optional[File3Processor]
            The file to be processed. One of `file` or `rawText` must be provided. Supported file types can be found [here](/developers/guides/supported-file-types).

        raw_text : typing.Optional[str]
            A raw string to be processed. Can be used in place of file when passing raw text data streams. One of `file` or `rawText` must be provided.

        priority : typing.Optional[int]
            An optional value used to determine the relative order of ProcessorRuns when rate limiting is in effect. Lower values will be prioritized before higher values.

        metadata : typing.Optional[JsonObject]
            An optional object that can be passed in to identify the run of the document processor. It will be returned back to you in the response and webhooks.

        config : typing.Optional[RunProcessorRequestConfig]
            The configuration for the processor run. If this is provided, this config will be used. If not provided, the config for the specific version you provide will be used. The type of configuration must match the processor type.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        RunProcessorResponse
            Successfully created processor run

        Examples
        --------
        import asyncio

        from extendconfig import AsyncExtend

        client = AsyncExtend(
            extend_api_version="YOUR_EXTEND_API_VERSION",
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.run_processor(
                processor_id="processor_id_here",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "processor_runs",
            method="POST",
            json={
                "processorId": processor_id,
                "version": version,
                "file": convert_and_respect_annotation_metadata(
                    object_=file, annotation=File3Processor, direction="write"
                ),
                "rawText": raw_text,
                "priority": priority,
                "metadata": metadata,
                "config": convert_and_respect_annotation_metadata(
                    object_=config, annotation=RunProcessorRequestConfig, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    RunProcessorResponse,
                    parse_obj_as(
                        type_=RunProcessorResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(
                        typing.Optional[typing.Any],
                        parse_obj_as(
                            type_=typing.Optional[typing.Any],  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 404:
                raise NotFoundError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def parse(
        self, *, file: ParseRequestFile, config: ParseConfig, request_options: typing.Optional[RequestOptions] = None
    ) -> ParseResponse:
        """
        Parse files to get cleaned, chunked target content (e.g. markdown).

        The Parse endpoint allows you to convert documents into structured, machine-readable formats with fine-grained control over the parsing process. This endpoint is ideal for extracting cleaned document content to be used as context for downstream processing, e.g. RAG pipelines, custom ingestion pipelines, embeddings classification, etc.

        Unlike processor and workflow runs, parsing is a synchronous endpoint and returns the parsed content in the response. Expected latency depends primarily on file size. This makes it suitable for workflows where you need immediate access to document content without waiting for asynchronous processing.

        For more details, see the [Parse File guide](/developers/guides/parse).

        Parameters
        ----------
        file : ParseRequestFile
            A file object containing either a URL or a fileId.

        config : ParseConfig

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ParseResponse
            Successfully parsed file

        Examples
        --------
        import asyncio

        from extendconfig import AsyncExtend, ParseConfig, ParseRequestFile

        client = AsyncExtend(
            extend_api_version="YOUR_EXTEND_API_VERSION",
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.parse(
                file=ParseRequestFile(),
                config=ParseConfig(),
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "parse",
            method="POST",
            json={
                "file": convert_and_respect_annotation_metadata(
                    object_=file, annotation=ParseRequestFile, direction="write"
                ),
                "config": convert_and_respect_annotation_metadata(
                    object_=config, annotation=ParseConfig, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ParseResponse,
                    parse_obj_as(
                        type_=ParseResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(
                        typing.Optional[typing.Any],
                        parse_obj_as(
                            type_=typing.Optional[typing.Any],  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        Error,
                        parse_obj_as(
                            type_=Error,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


def _get_base_url(*, base_url: typing.Optional[str] = None, environment: ExtendEnvironment) -> str:
    if base_url is not None:
        return base_url
    elif environment is not None:
        return environment.value
    else:
        raise Exception("Please pass in either base_url or environment to construct the client")
