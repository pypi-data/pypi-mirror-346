# hyperboost

–ü–æ–¥—Ö–æ–¥–∏—Ç –∫–∞–∫ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–¥–∞—á, —Ç–∞–∫ –∏ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö.


## ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–µ—Ç–æ–¥—ã

| –ú–µ—Ç–æ–¥                  | –û–ø–∏—Å–∞–Ω–∏–µ |
|------------------------|----------|
| `BayesianOptimizer`    | –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Gaussian Process |
| `EvolutionaryOptimizer` | –ü—Ä–æ—Å—Ç–æ–π –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º (GA) |


## üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞

```bash
pip install hyperoptlib
```

## üß™ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```python
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from hyperboost.space import SearchSpace
from hyperboost.space import Real, Integer, Categorical
from hyperboost.optimizers import BayesianOptimizer
from hyperboost.optimizers import EvolutionaryOptimizer

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

space = SearchSpace({
    "n_estimators": Integer(50, 300),
    "max_depth": Integer(3, 20),
    "criterion": Categorical(["gini", "entropy"]),
})

def objective(params):
    model = RandomForestClassifier(**params, random_state=42)
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    return -score  # –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –æ—à–∏–±–∫—É

# –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
bo = BayesianOptimizer(space, objective)
best_bo = bo.optimize(n_iter=30)
print("Bayesian best:", best_bo)

# –≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
ea = EvolutionaryOptimizer(space, objective)
best_ea = ea.optimize(population_size=20, generations=10)
print("Evolutionary best:", best_ea)
```

