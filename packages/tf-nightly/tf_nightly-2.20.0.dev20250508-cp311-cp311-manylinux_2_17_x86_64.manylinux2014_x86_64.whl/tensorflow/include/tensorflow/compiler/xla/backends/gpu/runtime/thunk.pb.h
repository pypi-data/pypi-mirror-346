// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: xla/backends/gpu/runtime/thunk.proto

#ifndef GOOGLE_PROTOBUF_INCLUDED_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto
#define GOOGLE_PROTOBUF_INCLUDED_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto

#include <limits>
#include <string>

#include <google/protobuf/port_def.inc>
#if PROTOBUF_VERSION < 3021000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers. Please update
#error your headers.
#endif
#if 3021009 < PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers. Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/port_undef.inc>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/metadata_lite.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/unknown_field_set.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>
#define PROTOBUF_INTERNAL_EXPORT_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto PROTOBUF_EXPORT
PROTOBUF_NAMESPACE_OPEN
namespace internal {
class AnyMetadata;
}  // namespace internal
PROTOBUF_NAMESPACE_CLOSE

// Internal implementation detail -- do not use these members.
struct PROTOBUF_EXPORT TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto {
  static const uint32_t offsets[];
};
PROTOBUF_EXPORT extern const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable descriptor_table_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
namespace xla {
namespace gpu {
class SequentialThunkProto;
struct SequentialThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern SequentialThunkProtoDefaultTypeInternal _SequentialThunkProto_default_instance_;
class ThunkInfoProto;
struct ThunkInfoProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern ThunkInfoProtoDefaultTypeInternal _ThunkInfoProto_default_instance_;
class ThunkProto;
struct ThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern ThunkProtoDefaultTypeInternal _ThunkProto_default_instance_;
}  // namespace gpu
}  // namespace xla
PROTOBUF_NAMESPACE_OPEN
template<> PROTOBUF_EXPORT ::xla::gpu::SequentialThunkProto* Arena::CreateMaybeMessage<::xla::gpu::SequentialThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::ThunkInfoProto* Arena::CreateMaybeMessage<::xla::gpu::ThunkInfoProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::ThunkProto* Arena::CreateMaybeMessage<::xla::gpu::ThunkProto>(Arena*);
PROTOBUF_NAMESPACE_CLOSE
namespace xla {
namespace gpu {

// ===================================================================

class PROTOBUF_EXPORT ThunkInfoProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.ThunkInfoProto) */ {
 public:
  inline ThunkInfoProto() : ThunkInfoProto(nullptr) {}
  ~ThunkInfoProto() override;
  explicit PROTOBUF_CONSTEXPR ThunkInfoProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ThunkInfoProto(const ThunkInfoProto& from);
  ThunkInfoProto(ThunkInfoProto&& from) noexcept
    : ThunkInfoProto() {
    *this = ::std::move(from);
  }

  inline ThunkInfoProto& operator=(const ThunkInfoProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline ThunkInfoProto& operator=(ThunkInfoProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ThunkInfoProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const ThunkInfoProto* internal_default_instance() {
    return reinterpret_cast<const ThunkInfoProto*>(
               &_ThunkInfoProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  friend void swap(ThunkInfoProto& a, ThunkInfoProto& b) {
    a.Swap(&b);
  }
  inline void Swap(ThunkInfoProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ThunkInfoProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ThunkInfoProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ThunkInfoProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ThunkInfoProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const ThunkInfoProto& from) {
    ThunkInfoProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ThunkInfoProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.ThunkInfoProto";
  }
  protected:
  explicit ThunkInfoProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kProfileAnnotationFieldNumber = 1,
    kExecutionStreamIdFieldNumber = 2,
  };
  // string profile_annotation = 1;
  void clear_profile_annotation();
  const std::string& profile_annotation() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_profile_annotation(ArgT0&& arg0, ArgT... args);
  std::string* mutable_profile_annotation();
  PROTOBUF_NODISCARD std::string* release_profile_annotation();
  void set_allocated_profile_annotation(std::string* profile_annotation);
  private:
  const std::string& _internal_profile_annotation() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_profile_annotation(const std::string& value);
  std::string* _internal_mutable_profile_annotation();
  public:

  // int64 execution_stream_id = 2;
  void clear_execution_stream_id();
  int64_t execution_stream_id() const;
  void set_execution_stream_id(int64_t value);
  private:
  int64_t _internal_execution_stream_id() const;
  void _internal_set_execution_stream_id(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:xla.gpu.ThunkInfoProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr profile_annotation_;
    int64_t execution_stream_id_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT ThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.ThunkProto) */ {
 public:
  inline ThunkProto() : ThunkProto(nullptr) {}
  ~ThunkProto() override;
  explicit PROTOBUF_CONSTEXPR ThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ThunkProto(const ThunkProto& from);
  ThunkProto(ThunkProto&& from) noexcept
    : ThunkProto() {
    *this = ::std::move(from);
  }

  inline ThunkProto& operator=(const ThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline ThunkProto& operator=(ThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ThunkProto& default_instance() {
    return *internal_default_instance();
  }
  enum ImplCase {
    kSequentialThunk = 2,
    IMPL_NOT_SET = 0,
  };

  static inline const ThunkProto* internal_default_instance() {
    return reinterpret_cast<const ThunkProto*>(
               &_ThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    1;

  friend void swap(ThunkProto& a, ThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(ThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const ThunkProto& from) {
    ThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.ThunkProto";
  }
  protected:
  explicit ThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kThunkInfoFieldNumber = 1,
    kSequentialThunkFieldNumber = 2,
  };
  // .xla.gpu.ThunkInfoProto thunk_info = 1;
  bool has_thunk_info() const;
  private:
  bool _internal_has_thunk_info() const;
  public:
  void clear_thunk_info();
  const ::xla::gpu::ThunkInfoProto& thunk_info() const;
  PROTOBUF_NODISCARD ::xla::gpu::ThunkInfoProto* release_thunk_info();
  ::xla::gpu::ThunkInfoProto* mutable_thunk_info();
  void set_allocated_thunk_info(::xla::gpu::ThunkInfoProto* thunk_info);
  private:
  const ::xla::gpu::ThunkInfoProto& _internal_thunk_info() const;
  ::xla::gpu::ThunkInfoProto* _internal_mutable_thunk_info();
  public:
  void unsafe_arena_set_allocated_thunk_info(
      ::xla::gpu::ThunkInfoProto* thunk_info);
  ::xla::gpu::ThunkInfoProto* unsafe_arena_release_thunk_info();

  // .xla.gpu.SequentialThunkProto sequential_thunk = 2;
  bool has_sequential_thunk() const;
  private:
  bool _internal_has_sequential_thunk() const;
  public:
  void clear_sequential_thunk();
  const ::xla::gpu::SequentialThunkProto& sequential_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::SequentialThunkProto* release_sequential_thunk();
  ::xla::gpu::SequentialThunkProto* mutable_sequential_thunk();
  void set_allocated_sequential_thunk(::xla::gpu::SequentialThunkProto* sequential_thunk);
  private:
  const ::xla::gpu::SequentialThunkProto& _internal_sequential_thunk() const;
  ::xla::gpu::SequentialThunkProto* _internal_mutable_sequential_thunk();
  public:
  void unsafe_arena_set_allocated_sequential_thunk(
      ::xla::gpu::SequentialThunkProto* sequential_thunk);
  ::xla::gpu::SequentialThunkProto* unsafe_arena_release_sequential_thunk();

  void clear_impl();
  ImplCase impl_case() const;
  // @@protoc_insertion_point(class_scope:xla.gpu.ThunkProto)
 private:
  class _Internal;
  void set_has_sequential_thunk();

  inline bool has_impl() const;
  inline void clear_has_impl();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::xla::gpu::ThunkInfoProto* thunk_info_;
    union ImplUnion {
      constexpr ImplUnion() : _constinit_{} {}
        ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
      ::xla::gpu::SequentialThunkProto* sequential_thunk_;
    } impl_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
    uint32_t _oneof_case_[1];

  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT SequentialThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.SequentialThunkProto) */ {
 public:
  inline SequentialThunkProto() : SequentialThunkProto(nullptr) {}
  ~SequentialThunkProto() override;
  explicit PROTOBUF_CONSTEXPR SequentialThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SequentialThunkProto(const SequentialThunkProto& from);
  SequentialThunkProto(SequentialThunkProto&& from) noexcept
    : SequentialThunkProto() {
    *this = ::std::move(from);
  }

  inline SequentialThunkProto& operator=(const SequentialThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline SequentialThunkProto& operator=(SequentialThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const SequentialThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const SequentialThunkProto* internal_default_instance() {
    return reinterpret_cast<const SequentialThunkProto*>(
               &_SequentialThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    2;

  friend void swap(SequentialThunkProto& a, SequentialThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(SequentialThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SequentialThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SequentialThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SequentialThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const SequentialThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const SequentialThunkProto& from) {
    SequentialThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(SequentialThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.SequentialThunkProto";
  }
  protected:
  explicit SequentialThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kThunksFieldNumber = 1,
  };
  // repeated .xla.gpu.ThunkProto thunks = 1;
  int thunks_size() const;
  private:
  int _internal_thunks_size() const;
  public:
  void clear_thunks();
  ::xla::gpu::ThunkProto* mutable_thunks(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto >*
      mutable_thunks();
  private:
  const ::xla::gpu::ThunkProto& _internal_thunks(int index) const;
  ::xla::gpu::ThunkProto* _internal_add_thunks();
  public:
  const ::xla::gpu::ThunkProto& thunks(int index) const;
  ::xla::gpu::ThunkProto* add_thunks();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto >&
      thunks() const;

  // @@protoc_insertion_point(class_scope:xla.gpu.SequentialThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto > thunks_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// ThunkInfoProto

// string profile_annotation = 1;
inline void ThunkInfoProto::clear_profile_annotation() {
  _impl_.profile_annotation_.ClearToEmpty();
}
inline const std::string& ThunkInfoProto::profile_annotation() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkInfoProto.profile_annotation)
  return _internal_profile_annotation();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void ThunkInfoProto::set_profile_annotation(ArgT0&& arg0, ArgT... args) {
 
 _impl_.profile_annotation_.Set(static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:xla.gpu.ThunkInfoProto.profile_annotation)
}
inline std::string* ThunkInfoProto::mutable_profile_annotation() {
  std::string* _s = _internal_mutable_profile_annotation();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkInfoProto.profile_annotation)
  return _s;
}
inline const std::string& ThunkInfoProto::_internal_profile_annotation() const {
  return _impl_.profile_annotation_.Get();
}
inline void ThunkInfoProto::_internal_set_profile_annotation(const std::string& value) {
  
  _impl_.profile_annotation_.Set(value, GetArenaForAllocation());
}
inline std::string* ThunkInfoProto::_internal_mutable_profile_annotation() {
  
  return _impl_.profile_annotation_.Mutable(GetArenaForAllocation());
}
inline std::string* ThunkInfoProto::release_profile_annotation() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkInfoProto.profile_annotation)
  return _impl_.profile_annotation_.Release();
}
inline void ThunkInfoProto::set_allocated_profile_annotation(std::string* profile_annotation) {
  if (profile_annotation != nullptr) {
    
  } else {
    
  }
  _impl_.profile_annotation_.SetAllocated(profile_annotation, GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (_impl_.profile_annotation_.IsDefault()) {
    _impl_.profile_annotation_.Set("", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.ThunkInfoProto.profile_annotation)
}

// int64 execution_stream_id = 2;
inline void ThunkInfoProto::clear_execution_stream_id() {
  _impl_.execution_stream_id_ = int64_t{0};
}
inline int64_t ThunkInfoProto::_internal_execution_stream_id() const {
  return _impl_.execution_stream_id_;
}
inline int64_t ThunkInfoProto::execution_stream_id() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkInfoProto.execution_stream_id)
  return _internal_execution_stream_id();
}
inline void ThunkInfoProto::_internal_set_execution_stream_id(int64_t value) {
  
  _impl_.execution_stream_id_ = value;
}
inline void ThunkInfoProto::set_execution_stream_id(int64_t value) {
  _internal_set_execution_stream_id(value);
  // @@protoc_insertion_point(field_set:xla.gpu.ThunkInfoProto.execution_stream_id)
}

// -------------------------------------------------------------------

// ThunkProto

// .xla.gpu.ThunkInfoProto thunk_info = 1;
inline bool ThunkProto::_internal_has_thunk_info() const {
  return this != internal_default_instance() && _impl_.thunk_info_ != nullptr;
}
inline bool ThunkProto::has_thunk_info() const {
  return _internal_has_thunk_info();
}
inline void ThunkProto::clear_thunk_info() {
  if (GetArenaForAllocation() == nullptr && _impl_.thunk_info_ != nullptr) {
    delete _impl_.thunk_info_;
  }
  _impl_.thunk_info_ = nullptr;
}
inline const ::xla::gpu::ThunkInfoProto& ThunkProto::_internal_thunk_info() const {
  const ::xla::gpu::ThunkInfoProto* p = _impl_.thunk_info_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::gpu::ThunkInfoProto&>(
      ::xla::gpu::_ThunkInfoProto_default_instance_);
}
inline const ::xla::gpu::ThunkInfoProto& ThunkProto::thunk_info() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.thunk_info)
  return _internal_thunk_info();
}
inline void ThunkProto::unsafe_arena_set_allocated_thunk_info(
    ::xla::gpu::ThunkInfoProto* thunk_info) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.thunk_info_);
  }
  _impl_.thunk_info_ = thunk_info;
  if (thunk_info) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.thunk_info)
}
inline ::xla::gpu::ThunkInfoProto* ThunkProto::release_thunk_info() {
  
  ::xla::gpu::ThunkInfoProto* temp = _impl_.thunk_info_;
  _impl_.thunk_info_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::gpu::ThunkInfoProto* ThunkProto::unsafe_arena_release_thunk_info() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.thunk_info)
  
  ::xla::gpu::ThunkInfoProto* temp = _impl_.thunk_info_;
  _impl_.thunk_info_ = nullptr;
  return temp;
}
inline ::xla::gpu::ThunkInfoProto* ThunkProto::_internal_mutable_thunk_info() {
  
  if (_impl_.thunk_info_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::gpu::ThunkInfoProto>(GetArenaForAllocation());
    _impl_.thunk_info_ = p;
  }
  return _impl_.thunk_info_;
}
inline ::xla::gpu::ThunkInfoProto* ThunkProto::mutable_thunk_info() {
  ::xla::gpu::ThunkInfoProto* _msg = _internal_mutable_thunk_info();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.thunk_info)
  return _msg;
}
inline void ThunkProto::set_allocated_thunk_info(::xla::gpu::ThunkInfoProto* thunk_info) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete _impl_.thunk_info_;
  }
  if (thunk_info) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(thunk_info);
    if (message_arena != submessage_arena) {
      thunk_info = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, thunk_info, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.thunk_info_ = thunk_info;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.ThunkProto.thunk_info)
}

// .xla.gpu.SequentialThunkProto sequential_thunk = 2;
inline bool ThunkProto::_internal_has_sequential_thunk() const {
  return impl_case() == kSequentialThunk;
}
inline bool ThunkProto::has_sequential_thunk() const {
  return _internal_has_sequential_thunk();
}
inline void ThunkProto::set_has_sequential_thunk() {
  _impl_._oneof_case_[0] = kSequentialThunk;
}
inline void ThunkProto::clear_sequential_thunk() {
  if (_internal_has_sequential_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.sequential_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::SequentialThunkProto* ThunkProto::release_sequential_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.sequential_thunk)
  if (_internal_has_sequential_thunk()) {
    clear_has_impl();
    ::xla::gpu::SequentialThunkProto* temp = _impl_.impl_.sequential_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.sequential_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::SequentialThunkProto& ThunkProto::_internal_sequential_thunk() const {
  return _internal_has_sequential_thunk()
      ? *_impl_.impl_.sequential_thunk_
      : reinterpret_cast< ::xla::gpu::SequentialThunkProto&>(::xla::gpu::_SequentialThunkProto_default_instance_);
}
inline const ::xla::gpu::SequentialThunkProto& ThunkProto::sequential_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.sequential_thunk)
  return _internal_sequential_thunk();
}
inline ::xla::gpu::SequentialThunkProto* ThunkProto::unsafe_arena_release_sequential_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.sequential_thunk)
  if (_internal_has_sequential_thunk()) {
    clear_has_impl();
    ::xla::gpu::SequentialThunkProto* temp = _impl_.impl_.sequential_thunk_;
    _impl_.impl_.sequential_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_sequential_thunk(::xla::gpu::SequentialThunkProto* sequential_thunk) {
  clear_impl();
  if (sequential_thunk) {
    set_has_sequential_thunk();
    _impl_.impl_.sequential_thunk_ = sequential_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.sequential_thunk)
}
inline ::xla::gpu::SequentialThunkProto* ThunkProto::_internal_mutable_sequential_thunk() {
  if (!_internal_has_sequential_thunk()) {
    clear_impl();
    set_has_sequential_thunk();
    _impl_.impl_.sequential_thunk_ = CreateMaybeMessage< ::xla::gpu::SequentialThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.sequential_thunk_;
}
inline ::xla::gpu::SequentialThunkProto* ThunkProto::mutable_sequential_thunk() {
  ::xla::gpu::SequentialThunkProto* _msg = _internal_mutable_sequential_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.sequential_thunk)
  return _msg;
}

inline bool ThunkProto::has_impl() const {
  return impl_case() != IMPL_NOT_SET;
}
inline void ThunkProto::clear_has_impl() {
  _impl_._oneof_case_[0] = IMPL_NOT_SET;
}
inline ThunkProto::ImplCase ThunkProto::impl_case() const {
  return ThunkProto::ImplCase(_impl_._oneof_case_[0]);
}
// -------------------------------------------------------------------

// SequentialThunkProto

// repeated .xla.gpu.ThunkProto thunks = 1;
inline int SequentialThunkProto::_internal_thunks_size() const {
  return _impl_.thunks_.size();
}
inline int SequentialThunkProto::thunks_size() const {
  return _internal_thunks_size();
}
inline void SequentialThunkProto::clear_thunks() {
  _impl_.thunks_.Clear();
}
inline ::xla::gpu::ThunkProto* SequentialThunkProto::mutable_thunks(int index) {
  // @@protoc_insertion_point(field_mutable:xla.gpu.SequentialThunkProto.thunks)
  return _impl_.thunks_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto >*
SequentialThunkProto::mutable_thunks() {
  // @@protoc_insertion_point(field_mutable_list:xla.gpu.SequentialThunkProto.thunks)
  return &_impl_.thunks_;
}
inline const ::xla::gpu::ThunkProto& SequentialThunkProto::_internal_thunks(int index) const {
  return _impl_.thunks_.Get(index);
}
inline const ::xla::gpu::ThunkProto& SequentialThunkProto::thunks(int index) const {
  // @@protoc_insertion_point(field_get:xla.gpu.SequentialThunkProto.thunks)
  return _internal_thunks(index);
}
inline ::xla::gpu::ThunkProto* SequentialThunkProto::_internal_add_thunks() {
  return _impl_.thunks_.Add();
}
inline ::xla::gpu::ThunkProto* SequentialThunkProto::add_thunks() {
  ::xla::gpu::ThunkProto* _add = _internal_add_thunks();
  // @@protoc_insertion_point(field_add:xla.gpu.SequentialThunkProto.thunks)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto >&
SequentialThunkProto::thunks() const {
  // @@protoc_insertion_point(field_list:xla.gpu.SequentialThunkProto.thunks)
  return _impl_.thunks_;
}

#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace gpu
}  // namespace xla

// @@protoc_insertion_point(global_scope)

#include <google/protobuf/port_undef.inc>
#endif  // GOOGLE_PROTOBUF_INCLUDED_GOOGLE_PROTOBUF_INCLUDED_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto
