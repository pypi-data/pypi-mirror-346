{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AymaraAI Accuracy Example\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Creating an eval with AymaraSDK\n",
    "- Fetching eval prompts\n",
    "- Calling OpenAI with those prompts\n",
    "- Creating an eval run with the responses\n",
    "\n",
    "## Requirements\n",
    "- Set `OPENAI_API_KEY` and `AYMARA_AI_API_KEY` in your environment or `.env` file.\n",
    "- Install dependencies: `pip install openai aymara-ai dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and imports\n",
    "import os\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from aymara_ai import AymaraAI\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not set in environment.\")\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the AymaraSDK client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AymaraAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_description = \"A helpful AI assistant.\"\n",
    "\n",
    "knowledge_base = open(\"aymara_language.txt\").read()\n",
    "\n",
    "eval_obj = client.evals.create(\n",
    "    ai_description=ai_description,\n",
    "    eval_type=\"accuracy\",\n",
    "    name=\"basic accuracy eval\",\n",
    "    ground_truth=knowledge_base,\n",
    "    num_prompts=5,\n",
    ")\n",
    "eval_id = eval_obj.eval_uuid\n",
    "eval_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch prompts for the eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymara_ai.lib.async_utils import wait_until_complete\n",
    "\n",
    "eval_obj = wait_until_complete(client.evals.get, resource_id=eval_id)\n",
    "eval_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from aymara_ai.types.eval_prompt import EvalPrompt\n",
    "\n",
    "prompts_response = client.evals.list_prompts(eval_id)\n",
    "prompts: List[EvalPrompt] = prompts_response.items\n",
    "prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call OpenAI for each prompt and collect responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from aymara_ai.types.eval_response_param import EvalResponseParam\n",
    "\n",
    "\n",
    "def answer_prompts(prompts: List[EvalPrompt]) -> List[EvalResponseParam]:\n",
    "    \"\"\"Answer the prompts using OpenAI's API.\"\"\"\n",
    "    responses: List[EvalResponseParam] = []\n",
    "    system_prompt = f\"\"\"<role>Assume this role for the following task: [{ai_description}].</role><task>Answer user questions using only the information in the knowledge base. If the knowledge base lacks the full answer to the question, then reply that you do not know the answer to the question. Do not share information outside the knowledge base.</task><knowledge_base>{knowledge_base}</knowledge_base>\"\"\"\n",
    "\n",
    "    for prompt in prompts:\n",
    "        prompt_text = prompt.content\n",
    "        prompt_uuid = prompt.prompt_uuid\n",
    "        completion = openai.responses.create(\n",
    "            model=\"o4-mini\",\n",
    "            instructions=system_prompt,\n",
    "            input=prompt_text,\n",
    "            max_output_tokens=256,\n",
    "        )\n",
    "        answer = completion.output_text\n",
    "        responses.append(EvalResponseParam(content=answer, prompt_uuid=prompt_uuid, ai_refused=not answer))\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = answer_prompts(prompts)\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an eval run with the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_run = client.evals.runs.create(eval_uuid=eval_id, responses=responses)\n",
    "eval_run_id = eval_run.eval_run_uuid\n",
    "eval_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_run = wait_until_complete(client.evals.runs.get, resource_id=eval_run_id)\n",
    "eval_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Report for the Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs = [eval_run]\n",
    "report = client.reports.create(eval_run_uuids=[run.eval_run_uuid for run in all_runs])\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = wait_until_complete(client.reports.get, resource_id=report.eval_suite_report_uuid)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [s.to_dict() for s in report.eval_run_reports]\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymara_ai.lib.plot import graph_eval_stats  # type: ignore\n",
    "\n",
    "graph_eval_stats(eval_runs=all_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
