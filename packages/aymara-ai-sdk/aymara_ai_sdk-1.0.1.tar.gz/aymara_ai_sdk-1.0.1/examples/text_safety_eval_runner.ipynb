{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# AymaraAI Text-to-Text Safety Eval with EvalRunner and AsyncEvalRunner\n",
    "\n",
    "This notebook demonstrates how to use both the synchronous `EvalRunner` and asynchronous `AsyncEvalRunner` for text-to-text safety evaluation with the AymaraAI SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "- Set `OPENAI_API_KEY` and `AYMARA_AI_API_KEY` in your environment or `.env` file.\n",
    "- Install dependencies:\n",
    "  ```bash\n",
    "  pip install openai aymara-ai dotenv\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and imports\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from aymara_ai import AymaraAI, AsyncAymaraAI\n",
    "from aymara_ai.lib.df import to_scores_df\n",
    "from aymara_ai.lib.runner import EvalRunner, AsyncEvalRunner\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## Define Model Callables\n",
    "\n",
    "The callable interface takes only a single argument: the prompt string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronous model callable for EvalRunner\n",
    "def openai_model_callable(prompt: str) -> str:\n",
    "    completion = openai.completions.create(\n",
    "        model=\"gpt-4.1-nano-2025-04-14\",\n",
    "        prompt=[prompt],\n",
    "        max_tokens=256,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return completion.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_openai_client = openai.AsyncOpenAI()\n",
    "\n",
    "\n",
    "# Asynchronous model callable for AsyncEvalRunner\n",
    "async def openai_model_callable_async(prompt: str) -> str:\n",
    "    completion = await async_openai_client.completions.create(\n",
    "        model=\"gpt-4.1-nano-2025-04-14\",\n",
    "        prompt=[prompt],\n",
    "        max_tokens=256,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return completion.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "## Define Eval Parameters\n",
    "\n",
    "We will use a basic text-to-text safety eval configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_description = \"A helpful AI assistant.\"\n",
    "ai_instructions = \"Please provide detailed answers to the prompts.\"\n",
    "\n",
    "eval_params = {\n",
    "    \"ai_description\": ai_description,\n",
    "    \"ai_instructions\": ai_instructions,\n",
    "    \"eval_type\": \"safety\",\n",
    "    \"num_prompts\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "## Synchronous Evaluation with EvalRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AymaraAI()\n",
    "runner = EvalRunner(client, openai_model_callable)\n",
    "eval_run = runner.run_eval(eval_params)\n",
    "display(f\"Eval Run ID: {eval_run.eval_run_uuid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "source": [
    "## Asynchronous Evaluation with AsyncEvalRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_client = AsyncAymaraAI()\n",
    "runner = AsyncEvalRunner(async_client, openai_model_callable_async)\n",
    "eval_run_async = await runner.run_eval(eval_params)\n",
    "\n",
    "\n",
    "display(f\"Async Eval Run ID: {eval_run_async.eval_run_uuid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "source": [
    "## Display and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for synchronous run\n",
    "prompts = client.evals.list_prompts(runner.eval_id).items\n",
    "responses = client.evals.runs.list_responses(runner.run_id).items\n",
    "to_scores_df(eval_run, prompts, responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b363d81ae4b689946ece5c682cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for async run\n",
    "async_client = AsyncAymaraAI()\n",
    "prompts_async = (await async_client.evals.list_prompts(eval_run_async.eval_uuid)).items\n",
    "responses_async = (await async_client.evals.runs.list_responses(eval_run_async.eval_run_uuid)).items\n",
    "to_scores_df(eval_run_async, prompts_async, responses_async)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65eabff63a45729fe45fb5ade58bdc",
   "metadata": {},
   "source": [
    "## (Optional) Visualize with graph_eval_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3933fab20d04ec698c2621248eb3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from aymara_ai.lib.plot import graph_eval_stats  # type: ignore\n",
    "\n",
    "    graph_eval_stats(eval_runs=[eval_run, eval_run_async])\n",
    "except ImportError:\n",
    "    display(\"Plotting utility not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4641cc4064e0191573fe9c69df29b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to use both the synchronous `EvalRunner` and asynchronous `AsyncEvalRunner` for text-to-text safety evaluation with the AymaraAI SDK, using the updated callable interface. Use the synchronous runner for simple, blocking workflows, and the async runner for scalable or concurrent evaluation tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
