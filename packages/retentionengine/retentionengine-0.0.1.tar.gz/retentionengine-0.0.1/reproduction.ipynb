{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install transformers accelerate bitsandbytes",
   "id": "4d416a53778f67df",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from lattent import TTTForCausalLM, TTTConfig, TTT_STANDARD_CONFIGS\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set CUDA Device\n",
    "device_num = 0\n",
    "\n",
    "if torch.cuda.is_available() and device_num != -1:\n",
    "    torch.cuda.set_device(device_num)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_num = -1  # cpu\n",
    "print(f\"INFO: Using device - {device}:{device_num}\")"
   ],
   "id": "7a6b9a138cd45858",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_id = \"meta-llama/Llama-2-7b-hf\"",
   "id": "43ebd8612c6db748",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Common Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "id": "1585f22ab1a99eb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initializing a TTT ttt-1b style configuration\n",
    "# configuration = TTTConfig(**TTT_STANDARD_CONFIGS['1b']) is equivalent to the following\n",
    "configuration = TTTConfig()\n",
    "configuration"
   ],
   "id": "f4037d0093f31bbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initializing a model from the ttt-1b style configuration\n",
    "model = TTTForCausalLM(configuration)\n",
    "model.to(device)\n",
    "model.eval()"
   ],
   "id": "3e9fb755529c52ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_text = \"Greeting from TTT!\"\n",
    "\n",
    "inf_params = dict(\n",
    "    input_ids=tokenizer(input_text, return_tensors=\"pt\").to(device).input_ids,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ],
   "id": "d89110fd3d89b9f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inference using TTT\n",
    "with torch.no_grad():\n",
    "    out_ids = model.generate(**inf_params)\n",
    "    print(*tokenizer.batch_decode(out_ids, skip_special_tokens=True))"
   ],
   "id": "fb4a7b1b3d6ee9ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "73a0a71d4816609f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
