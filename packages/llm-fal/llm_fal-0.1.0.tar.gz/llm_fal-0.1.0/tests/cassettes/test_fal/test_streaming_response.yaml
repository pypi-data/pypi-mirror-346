interactions:
- request:
    body: '{"prompt": "Tell me a story", "temperature": 0.7, "stream": true}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '65'
      content-type:
      - application/json
      host:
      - api.fal.ai
      user-agent:
      - python-requests/2.31.0
    method: POST
    uri: https://api.fal.ai/v1/models/fal-ai/text-generation/llm/inference
  response:
    body:
      string: '{"data": {"text": "This "}}
{"data": {"text": "is "}}
{"data": {"text": "a "}}
{"data": {"text": "streaming "}}
{"data": {"text": "response "}}
{"data": {"text": "from "}}
{"data": {"text": "fal.ai."}}'
    headers:
      content-type:
      - application/x-ndjson
      date:
      - Sat, 19 Apr 2025 12:35:45 GMT
      server:
      - uvicorn
      transfer-encoding:
      - chunked
    status:
      code: 200
      message: OK
version: 1
